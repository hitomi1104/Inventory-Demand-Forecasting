{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Future Sales Using Machine Learning\n",
    "\n",
    "Forecasting future sales of a product offers many advantages. Predicting future sales of a product helps a company manage the cost of manufacturing and marketing the product. In this notebook, I will try to you through the task of future sales prediction with machine learning using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:28.334615Z",
     "iopub.status.busy": "2022-09-28T19:11:28.333232Z",
     "iopub.status.idle": "2022-09-28T19:11:36.257768Z",
     "shell.execute_reply": "2022-09-28T19:11:36.256584Z",
     "shell.execute_reply.started": "2022-09-28T19:11:28.334479Z"
    }
   },
   "outputs": [],
   "source": [
    "# EDA Libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "import matplotlib.colors as col\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path  \n",
    "import random\n",
    "\n",
    "# Scikit-Learn models:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import catboost as cbt\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LSTM:\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# ARIMA Model:\n",
    "\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "\n",
    "import pickle\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.260548Z",
     "iopub.status.busy": "2022-09-28T19:11:36.259813Z",
     "iopub.status.idle": "2022-09-28T19:11:36.265685Z",
     "shell.execute_reply": "2022-09-28T19:11:36.264462Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.260514Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Dataset & Data Exploration (EDA)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The first step is to load the data and transform it into a structure that we will then use for each of our models. In its raw form, each row of data represents a single day of sales at one of ten stores. Our goal is to predict monthly sales, so we will first consolidate all stores and days into total monthly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.267634Z",
     "iopub.status.busy": "2022-09-28T19:11:36.26715Z",
     "iopub.status.idle": "2022-09-28T19:11:36.33136Z",
     "shell.execute_reply": "2022-09-28T19:11:36.330112Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.267578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  sales\n",
       "0   0     52\n",
       "1   1     52\n",
       "2   2     52\n",
       "3   3     52\n",
       "4   4     52"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/sample_submission.csv')\n",
    "df = dataset.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.334911Z",
     "iopub.status.busy": "2022-09-28T19:11:36.334422Z",
     "iopub.status.idle": "2022-09-28T19:11:36.340923Z",
     "shell.execute_reply": "2022-09-28T19:11:36.339757Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.334865Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"Returns a pandas dataframe from a csv file.\"\"\"\n",
    "#     return pd.read_csv(file_name)\n",
    "    return pd.read_excel(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.343382Z",
     "iopub.status.busy": "2022-09-28T19:11:36.34287Z",
     "iopub.status.idle": "2022-09-28T19:11:37.050788Z",
     "shell.execute_reply": "2022-09-28T19:11:37.04965Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.343331Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales_data = load_data('data/data.xlsx')\n",
    "df_s = sales_data.copy()\n",
    "\n",
    "df_s = df_s[[\"CreatedOnUtc\", \"Quantity\", \"Sku\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Date: 2023-03-03 00:00:00\n",
      "Maximum Date: 2024-05-03 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/t7zr8nds6f727x199sygjpbh0000gn/T/ipykernel_8069/1140341070.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_s['CreatedOnUtc'] = pd.to_datetime(df_s['CreatedOnUtc'])\n"
     ]
    }
   ],
   "source": [
    "df_s['CreatedOnUtc'] = pd.to_datetime(df_s['CreatedOnUtc'])\n",
    "min_date = df_s['CreatedOnUtc'].min()\n",
    "max_date = df_s['CreatedOnUtc'].max()\n",
    "\n",
    "print(\"Minimum Date:\", min_date)\n",
    "print(\"Maximum Date:\", max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sku(number):\n",
    "    return f'SKU{number}'\n",
    "\n",
    "# Applying the function to the 'store' column\n",
    "df_s['store'] = df_s['store'].apply(convert_to_sku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.052694Z",
     "iopub.status.busy": "2022-09-28T19:11:37.052373Z",
     "iopub.status.idle": "2022-09-28T19:11:37.06468Z",
     "shell.execute_reply": "2022-09-28T19:11:37.063493Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.052666Z"
    }
   },
   "outputs": [],
   "source": [
    "df_s.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.066938Z",
     "iopub.status.busy": "2022-09-28T19:11:37.066191Z",
     "iopub.status.idle": "2022-09-28T19:11:37.117364Z",
     "shell.execute_reply": "2022-09-28T19:11:37.116195Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.066895Z"
    }
   },
   "outputs": [],
   "source": [
    "# To view basic statistical details about dataset:\n",
    "\n",
    "df_s['sales'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <h4 style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Sales seem to be unbalanced!\n",
    "    </h4>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.120067Z",
     "iopub.status.busy": "2022-09-28T19:11:37.1196Z",
     "iopub.status.idle": "2022-09-28T19:11:37.650172Z",
     "shell.execute_reply": "2022-09-28T19:11:37.649108Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.120012Z"
    }
   },
   "outputs": [],
   "source": [
    "df_s['sales'].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Returns a dataframe where each row represents total sales for a given month. Columns include 'date' by month and 'sales'.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.651889Z",
     "iopub.status.busy": "2022-09-28T19:11:37.651552Z",
     "iopub.status.idle": "2022-09-28T19:11:37.659058Z",
     "shell.execute_reply": "2022-09-28T19:11:37.657963Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.651858Z"
    }
   },
   "outputs": [],
   "source": [
    "def monthlyORyears_sales(data,time=['monthly','years']):\n",
    "    data = data.copy()\n",
    "    if time == \"monthly\":\n",
    "        # Drop the day indicator from the date column:\n",
    "        data.date = data.date.apply(lambda x: str(x)[:-3])\n",
    "    else:\n",
    "        data.date = data.date.apply(lambda x: str(x)[:4])\n",
    "        \n",
    "   # Sum sales per month: \n",
    "    data = data.groupby('date')['sales'].sum().reset_index()\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "        \n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.662972Z",
     "iopub.status.busy": "2022-09-28T19:11:37.662607Z",
     "iopub.status.idle": "2022-09-28T19:11:38.171218Z",
     "shell.execute_reply": "2022-09-28T19:11:38.170417Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.662928Z"
    }
   },
   "outputs": [],
   "source": [
    "m_df = monthlyORyears_sales(df_s,\"monthly\")\n",
    "\n",
    "m_df.to_csv('./monthly_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:38.173179Z",
     "iopub.status.busy": "2022-09-28T19:11:38.172628Z",
     "iopub.status.idle": "2022-09-28T19:11:38.183395Z",
     "shell.execute_reply": "2022-09-28T19:11:38.182255Z",
     "shell.execute_reply.started": "2022-09-28T19:11:38.173145Z"
    }
   },
   "outputs": [],
   "source": [
    "m_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In our new data frame, each row now represents the total sales for a given month across stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:38.184808Z",
     "iopub.status.busy": "2022-09-28T19:11:38.184519Z",
     "iopub.status.idle": "2022-09-28T19:11:38.676214Z",
     "shell.execute_reply": "2022-09-28T19:11:38.675384Z",
     "shell.execute_reply.started": "2022-09-28T19:11:38.184782Z"
    }
   },
   "outputs": [],
   "source": [
    "y_df = monthlyORyears_sales(df_s,\"years\")\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:38.67754Z",
     "iopub.status.busy": "2022-09-28T19:11:38.67722Z",
     "iopub.status.idle": "2022-09-28T19:11:39.19411Z",
     "shell.execute_reply": "2022-09-28T19:11:39.192798Z",
     "shell.execute_reply.started": "2022-09-28T19:11:38.677512Z"
    }
   },
   "outputs": [],
   "source": [
    "layout = (1, 2)\n",
    "\n",
    "raw = plt.subplot2grid(layout, (0 ,0))\n",
    "law = plt.subplot2grid(layout, (0 ,1))\n",
    "\n",
    "years = y_df['sales'].plot(kind = \"bar\",color = 'mediumblue', label=\"Sales\",ax=raw, figsize=(12,5))\n",
    "months = m_df['sales'].plot(marker = 'o',color = 'darkorange', label=\"Sales\", ax=law)\n",
    "\n",
    "years.set(xlabel = \"Years\",title = \"Distribution of Sales Per Year\")\n",
    "months.set(xlabel = \"Months\", title = \"Distribution of Sales Per Mounth\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "years.legend()\n",
    "months.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <p style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ If we plot the total monthly sales over time, we see that the average monthly sales increase over time, so our data is not stationary.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ðŸ¦Ž TREND AND SEASONALITY](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n",
    "> There are many other models to forecast time series, such as weighted\n",
    "moving average models or autoregressive integrated moving average\n",
    "(ARIMA) models. Some of them require you to first remove the trend\n",
    "and seasonality. For example, if you are studying the number of active\n",
    "users on your website, and it is growing by 10% every month, you\n",
    "would have to remove this trend from the time series. Once the model\n",
    "is trained and starts making predictions, you would have to add the\n",
    "trend back to get the final predictions. Similarly, if you are trying to\n",
    "predict the amount of sunscreen lotion sold every month, you will\n",
    "probably observe strong seasonality: since it sells well every summer,\n",
    "a similar pattern will be repeated every year. You would have to\n",
    "remove this seasonality from the time series, for example by\n",
    "computing the difference between the value at each time step and the\n",
    "value one year earlier (this technique is called differencing). Again,\n",
    "after the model is trained and makes predictions, you would have to\n",
    "add the seasonal pattern back to get the final predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Exploration (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To make it stationary, we will calculate the difference between the sales of each month and add it to our data frame as a new column.\n",
    "> Further details of the stationary and differences are available [here.](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/) But first let's take a closer look at the data set you work with by learning more about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.195933Z",
     "iopub.status.busy": "2022-09-28T19:11:39.195555Z",
     "iopub.status.idle": "2022-09-28T19:11:39.202363Z",
     "shell.execute_reply": "2022-09-28T19:11:39.201059Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.1959Z"
    }
   },
   "outputs": [],
   "source": [
    "def sales_time(data):\n",
    "    \"\"\"Time interval of dataset:\"\"\"\n",
    "\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "    n_of_days = data.date.max() - data.date.min()\n",
    "    n_of_years = int(n_of_days.days / 365)\n",
    "    \n",
    "    print(f\"Days: {n_of_days.days}\\nYears: {n_of_years}\\nMonth: {12 * n_of_years}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.20501Z",
     "iopub.status.busy": "2022-09-28T19:11:39.204548Z",
     "iopub.status.idle": "2022-09-28T19:11:39.354247Z",
     "shell.execute_reply": "2022-09-28T19:11:39.353037Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.204968Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_time(df_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sales Data Per Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.356381Z",
     "iopub.status.busy": "2022-09-28T19:11:39.355978Z",
     "iopub.status.idle": "2022-09-28T19:11:39.364014Z",
     "shell.execute_reply": "2022-09-28T19:11:39.363068Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.356332Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sales_per_store(data):\n",
    "    sales_by_store = data.groupby('store')['sales'].sum().reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    sns.barplot(x=sales_by_store['store'], y=sales_by_store['sales'], color='darkred')\n",
    "    \n",
    "    ax.set(xlabel=\"Store Id\", ylabel=\"Sum of Sales\", title=\"Total Sales Per Store\")\n",
    "    \n",
    "    return sales_by_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.36568Z",
     "iopub.status.busy": "2022-09-28T19:11:39.36518Z",
     "iopub.status.idle": "2022-09-28T19:11:39.664247Z",
     "shell.execute_reply": "2022-09-28T19:11:39.663088Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.365647Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_per_store(df_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Monthly Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.666317Z",
     "iopub.status.busy": "2022-09-28T19:11:39.665827Z",
     "iopub.status.idle": "2022-09-28T19:11:39.674328Z",
     "shell.execute_reply": "2022-09-28T19:11:39.67322Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.666247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Overall for 5 years:\n",
    "\n",
    "average_m_sales = m_df.sales.mean()\n",
    "print(f\"Overall Avarage Monthly Sales: ${average_m_sales}\")\n",
    "\n",
    "def avarage_12months():\n",
    "# Last 1 years (this will be the forecasted sales):\n",
    "    average_m_sales_1y = m_df.sales[-12:].mean()\n",
    "    print(f\"Last 12 months average monthly sales: ${average_m_sales_1y}\")\n",
    "avarage_12months()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Determining Time Series Stationary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented in this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.676324Z",
     "iopub.status.busy": "2022-09-28T19:11:39.67573Z",
     "iopub.status.idle": "2022-09-28T19:11:39.689424Z",
     "shell.execute_reply": "2022-09-28T19:11:39.688367Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.676278Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_plot(data, x_col, y_col, title):\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    sns.lineplot(x=x_col, y=y_col, data=data, ax=ax, color='darkblue', label='Total Sales')\n",
    "    \n",
    "    # Calculating the yearly mean of sales\n",
    "    s_mean = data.groupby(data[x_col].dt.year)[y_col].mean().reset_index()\n",
    "    s_mean[x_col] = pd.to_datetime(s_mean[x_col], format='%Y')  # Adjust this if the format doesn't match\n",
    "\n",
    "    # Plotting the yearly mean\n",
    "    sns.lineplot(x=s_mean[x_col], y=s_mean[y_col], ax=ax, color='darkred', label='Yearly Average Sales')\n",
    "\n",
    "    ax.set(title=title, xlabel=x_col, ylabel=y_col)\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.691874Z",
     "iopub.status.busy": "2022-09-28T19:11:39.691161Z",
     "iopub.status.idle": "2022-09-28T19:11:40.068119Z",
     "shell.execute_reply": "2022-09-28T19:11:40.066881Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.691831Z"
    }
   },
   "outputs": [],
   "source": [
    "time_plot(m_df, 'date', 'sales', 'Monthly Sales Before Diff Transformation' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Differencing**\n",
    "\n",
    "In this method, we compute the difference of consecutive terms in the series. Differencing is typically performed to get rid of the varying mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.069828Z",
     "iopub.status.busy": "2022-09-28T19:11:40.069482Z",
     "iopub.status.idle": "2022-09-28T19:11:40.076522Z",
     "shell.execute_reply": "2022-09-28T19:11:40.075224Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.069799Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_diff(data):\n",
    "    \"\"\"Calculate the difference in sales month over month:\"\"\"\n",
    "    \n",
    "    data['sales_diff'] = data.sales.diff()\n",
    "    data = data.dropna()\n",
    "    \n",
    "    data.to_csv('./stationary_df.csv')\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.079688Z",
     "iopub.status.busy": "2022-09-28T19:11:40.078706Z",
     "iopub.status.idle": "2022-09-28T19:11:40.447236Z",
     "shell.execute_reply": "2022-09-28T19:11:40.446366Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.079644Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stationary_df = get_diff(m_df)\n",
    "time_plot(stationary_df, 'date', 'sales_diff', \n",
    "          'Monthly Sales After Diff Transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <h5 style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Now that our data represent monthly sales and we have transformed it to be stationary, we will set up the \n",
    "                            data for our different model types. <br> <br>\n",
    "              âœ” To do this, we will \n",
    "                            define two different structures: \n",
    "        <br> <br>\n",
    "                    1. one will be used for ARIMA modeling,\n",
    "        <br><br>\n",
    "                    2. the other will be used for the rest of the models.\n",
    "    </h5>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing Dataset Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling\n",
    "\n",
    ">  For our Arima model, we will need only a datetime index and the dependent variable (diff in sales) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.448968Z",
     "iopub.status.busy": "2022-09-28T19:11:40.448314Z",
     "iopub.status.idle": "2022-09-28T19:11:40.454539Z",
     "shell.execute_reply": "2022-09-28T19:11:40.453661Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.448935Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_arima_data(data):\n",
    "    \"\"\"Generates a csv-file with a datetime index and a dependent sales column for ARIMA modeling.\"\"\"\n",
    "    \n",
    "    da_data = data.set_index('date').drop('sales', axis=1)\n",
    "    da_data.dropna(axis=0)\n",
    "    \n",
    "    da_data.to_csv('./arima_df.csv')\n",
    "    \n",
    "    return da_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.457149Z",
     "iopub.status.busy": "2022-09-28T19:11:40.455979Z",
     "iopub.status.idle": "2022-09-28T19:11:40.483662Z",
     "shell.execute_reply": "2022-09-28T19:11:40.482544Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.457065Z"
    }
   },
   "outputs": [],
   "source": [
    "datatime_df = build_arima_data(stationary_df)\n",
    "datatime_df # ARIMA Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Observing Lags**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our other models, we will create a new data frame where each feature represents a previous monthâ€™s sales. To determine how many months to include in our feature set, we will observe the autocorrelation and partial autocorrelation plots and use the [rules for selecting lags in ARIMA modeling](https://people.duke.edu/~rnau/arimrule.htm). This way, we can keep consistent a look-back period for our ARIMA and regressive models.\n",
    "\n",
    ">  **[Statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.485415Z",
     "iopub.status.busy": "2022-09-28T19:11:40.48506Z",
     "iopub.status.idle": "2022-09-28T19:11:41.117606Z",
     "shell.execute_reply": "2022-09-28T19:11:41.116363Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.485385Z"
    }
   },
   "outputs": [],
   "source": [
    "def plots_lag(data, lags=None):\n",
    "    \"\"\"Convert dataframe to datetime index\"\"\"\n",
    "    dt_data = data.set_index('date').drop('sales', axis=1)\n",
    "    dt_data.dropna(axis=0)\n",
    "    \n",
    "    \n",
    "    law  = plt.subplot(122)\n",
    "    acf  = plt.subplot(221)\n",
    "    pacf = plt.subplot(223)\n",
    "    \n",
    "    dt_data.plot(ax=law, figsize=(10, 5), color='orange')\n",
    "    # Plot the autocorrelation function:\n",
    "    smt.graphics.plot_acf(dt_data, lags=lags, ax=acf, color='mediumblue')\n",
    "    smt.graphics.plot_pacf(dt_data, lags=lags, ax=pacf, color='mediumblue')\n",
    "    \n",
    "    # Will also adjust spacing between subplots to minimize the overlaps:\n",
    "    plt.tight_layout()\n",
    "\n",
    "plots_lag(stationary_df, lags=24);\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> [ðŸŽ¦ Autocorrelation Function](https://www.youtube.com/watch?v=ZjaBn93YPWo&t=12s)\n",
    "\n",
    "\n",
    ">[ Time Series & Autocorrelation](https://online.stat.psu.edu/stat501/book/export/html/995 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <p style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Based on the above, we will choose our look-back period to be 12 months. We will, therefore, generate a data frame that has 13 columns, 1 column for each of the 12 months and the column for our dependent variable, difference in sales.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressive Modeling\n",
    "\n",
    "> Let's create a CSV file where each row represents a month and the columns will have sales, dependent variables, and previous sales for each delay. The 12 delay properties are created according to the EDA. Data is used for regression modeling.\n",
    "\n",
    "\n",
    "Code was taken from [Baris Karamanâ€™s 'Data Driven Growth' series.](https://towardsdatascience.com/predicting-sales-611cb5a252de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.11967Z",
     "iopub.status.busy": "2022-09-28T19:11:41.119148Z",
     "iopub.status.idle": "2022-09-28T19:11:41.126284Z",
     "shell.execute_reply": "2022-09-28T19:11:41.125263Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.119636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a data frame for transformation from time series to supervised:\n",
    "\n",
    "def built_supervised(data):\n",
    "    supervised_df = data.copy()\n",
    "\n",
    "    # Create column for each lag:\n",
    "    for i in range(1, 13):\n",
    "        col_name = 'lag_' + str(i)\n",
    "        supervised_df[col_name] = supervised_df['sales_diff'].shift(i)\n",
    "\n",
    "    # Drop null values:\n",
    "    supervised_df = supervised_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    supervised_df.to_csv('./model_df.csv', index=False)\n",
    "    \n",
    "    return supervised_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.128469Z",
     "iopub.status.busy": "2022-09-28T19:11:41.127519Z",
     "iopub.status.idle": "2022-09-28T19:11:41.225335Z",
     "shell.execute_reply": "2022-09-28T19:11:41.22412Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.128435Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df = built_supervised(stationary_df)\n",
    "model_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.231702Z",
     "iopub.status.busy": "2022-09-28T19:11:41.23131Z",
     "iopub.status.idle": "2022-09-28T19:11:41.247532Z",
     "shell.execute_reply": "2022-09-28T19:11:41.246164Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.231666Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df.info() # Supervised Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <h5 style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Now we have two separate data structures: <br> <br><br>\n",
    "              âœ” Our Arima structure which includes a DataTime index,\n",
    "        <br><br>\n",
    "             âœ” Our supervised structure which includes lags as features.\n",
    "    </h5>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions For Modeling** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and assess all of our models, we will use a series of helper functions that perform the following functions:\n",
    "* *Train test split*\n",
    "* *Scale the data*\n",
    "* *Reverse scaling*\n",
    "* *Create a predictions data frame*\n",
    "* *Score the models*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Train Test Split\n",
    "\n",
    ">       We detach our data so that the last 12 months are part of the test set and the rest of the data is used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.250036Z",
     "iopub.status.busy": "2022-09-28T19:11:41.249672Z",
     "iopub.status.idle": "2022-09-28T19:11:41.261549Z",
     "shell.execute_reply": "2022-09-28T19:11:41.260073Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.250004Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    data = data.drop(['sales','date'], axis=1)\n",
    "    train , test = data[:-12].values, data[-12:].values\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(model_df)\n",
    "print(f\"Shape of  Train: {train.shape}\\nShape of  Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Scale The Data\n",
    "\n",
    ">      Using a min-max scaler, we will scale the data so that all of our variables fall within the range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.263859Z",
     "iopub.status.busy": "2022-09-28T19:11:41.263005Z",
     "iopub.status.idle": "2022-09-28T19:11:41.276472Z",
     "shell.execute_reply": "2022-09-28T19:11:41.275161Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.263803Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_data(train_set,test_set):\n",
    "    \"\"\"Scales data using MinMaxScaler and separates data into X_train, y_train,\n",
    "    X_test, and y_test.\"\"\"\n",
    "    \n",
    "    # Apply Min Max Scaler:\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train_set)\n",
    "    \n",
    "    # Reshape training set:\n",
    "    train_set = train_set.reshape(train_set.shape[0],\n",
    "                                  train_set.shape[1])\n",
    "    train_set_scaled = scaler.transform(train_set)\n",
    "    \n",
    "    # Reshape test set:\n",
    "    test_set = test_set.reshape(test_set.shape[0], \n",
    "                                test_set.shape[1])\n",
    "    test_set_scaled = scaler.transform(test_set)\n",
    "    \n",
    "    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel() # returns the array, flattened!\n",
    "    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_object = scale_data(train, test)\n",
    "print(f\"Shape of X Train: {X_train.shape}\\nShape of y Train: {y_train.shape}\\nShape of X Test: {X_test.shape}\\nShape of y Test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Reverse Scaling\n",
    ">       After running our models, we will use this helper function to reverse the scaling of step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.278892Z",
     "iopub.status.busy": "2022-09-28T19:11:41.278517Z",
     "iopub.status.idle": "2022-09-28T19:11:41.28886Z",
     "shell.execute_reply": "2022-09-28T19:11:41.287371Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.278859Z"
    }
   },
   "outputs": [],
   "source": [
    "def re_scaling(y_pred, x_test, scaler_obj, lstm=False):\n",
    "    \"\"\"For visualizing and comparing results, undoes the scaling effect on predictions.\"\"\"\n",
    "   # y_pred: model predictions\n",
    "   # x_test: features from the test set used for predictions\n",
    "   # scaler_obj: the scaler objects used for min-max scaling\n",
    "   # lstm: indicate if the model run is the lstm. If True, additional transformation occurs \n",
    "    \n",
    "    # Reshape y_pred:\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0],\n",
    "                            1,\n",
    "                            1)\n",
    "\n",
    "    if not lstm:\n",
    "        x_test = x_test.reshape(x_test.shape[0],\n",
    "                                1, \n",
    "                                x_test.shape[1])\n",
    "\n",
    "    # Rebuild test set for inverse transform:\n",
    "    pred_test_set = []\n",
    "    for index in range(0, len(y_pred)):\n",
    "        pred_test_set.append(np.concatenate([y_pred[index], \n",
    "                                             x_test[index]],\n",
    "                                             axis=1) )\n",
    "\n",
    "    # Reshape pred_test_set:\n",
    "    pred_test_set = np.array(pred_test_set)\n",
    "    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0],\n",
    "                                          pred_test_set.shape[2])\n",
    "\n",
    "    # Inverse transform:\n",
    "    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)\n",
    "\n",
    "    return pred_test_set_inverted\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Predictions Dataframe\n",
    ">     Generate a dataframe that includes the actual sales captured in our test set and the predicted results from our model so that we can quantify our success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.290989Z",
     "iopub.status.busy": "2022-09-28T19:11:41.290531Z",
     "iopub.status.idle": "2022-09-28T19:11:41.305596Z",
     "shell.execute_reply": "2022-09-28T19:11:41.304677Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.290946Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_df(unscale_predictions, origin_df):\n",
    "    \"\"\"Generates a dataframe that shows the predicted sales for each month\n",
    "    for plotting results.\"\"\"\n",
    "    \n",
    "    # unscale_predictions: the model predictions that do not have min-max or other scaling applied\n",
    "    # origin_df: the original monthly sales dataframe\n",
    "    \n",
    "    # Create dataframe that shows the predicted sales:\n",
    "    result_list = []\n",
    "    sales_dates = list(origin_df[-13:].date)\n",
    "    act_sales = list(origin_df[-13:].sales)\n",
    "\n",
    "    for index in range(0, len(unscale_predictions)):\n",
    "        result_dict = {}\n",
    "        result_dict['pred_value'] = int(unscale_predictions[index][0] + act_sales[index])\n",
    "        result_dict['date'] = sales_dates[index + 1]\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score The Models\n",
    ">      This helper function will save the root mean squared error (RMSE) and mean absolute error (MAE) of our predictions to compare the performance of our models.\n",
    "[Regression Metrics](https://scikit-learn.org/0.24/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://4.bp.blogspot.com/-wG7IbjTfE6k/XGUvqm7TCVI/AAAAAAAAAZU/vpH1kuKTIooKTcVlnm1EVRCXLVZM9cPNgCLcBGAs/s1600/formula-MAE-MSE-RMSE-RSquared.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.306909Z",
     "iopub.status.busy": "2022-09-28T19:11:41.306602Z",
     "iopub.status.idle": "2022-09-28T19:11:41.322428Z",
     "shell.execute_reply": "2022-09-28T19:11:41.321372Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.306882Z"
    }
   },
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "\n",
    "def get_scores(unscale_df, origin_df, model_name):\n",
    "    \"\"\"Prints the root mean squared error, mean absolute error, and r2 scores\n",
    "    for each model. Saves all results in a model_scores dictionary for\n",
    "    comparison.\"\"\"\n",
    "\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(origin_df.sales[-12:], \n",
    "                                      unscale_df.pred_value[-12:]))\n",
    "    \n",
    "    mae = mean_absolute_error(origin_df.sales[-12:], \n",
    "                              unscale_df.pred_value[-12:])\n",
    "    \n",
    "    r2 = r2_score(origin_df.sales[-12:], \n",
    "                  unscale_df.pred_value[-12:])\n",
    "    \n",
    "    model_scores[model_name] = [rmse, mae, r2]\n",
    "\n",
    "    print(f\"RMSE: {rmse}\\nMAE: {mae}\\nR2 Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.325278Z",
     "iopub.status.busy": "2022-09-28T19:11:41.324106Z",
     "iopub.status.idle": "2022-09-28T19:11:41.335806Z",
     "shell.execute_reply": "2022-09-28T19:11:41.334039Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.325235Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_results(results, origin_df, model_name):\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    sns.lineplot(x='date', y='sales', data=origin_df, ax=ax, label='Original', color='blue')\n",
    "    sns.lineplot(x='date', y='pred_value', data=results, ax=ax, label='Predicted', color='red')\n",
    "    \n",
    "    ax.set(xlabel=\"Date\", ylabel=\"Sales\", title=f\"{model_name} Sales Forecasting Prediction\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    filepath = Path('./model_output/{model_name}_forecasting.svg')  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True) \n",
    "    plt.savefig(f'./model_output/{model_name}_forecasting.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.337847Z",
     "iopub.status.busy": "2022-09-28T19:11:41.337443Z",
     "iopub.status.idle": "2022-09-28T19:11:41.352859Z",
     "shell.execute_reply": "2022-09-28T19:11:41.351858Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.337816Z"
    }
   },
   "outputs": [],
   "source": [
    "def regressive_model(train_data, test_data, model, model_name):\n",
    "    \"\"\"Runs regressive models in SKlearn framework. First calls scale_data\n",
    "    to split into X and y and scale the data. Then fits and predicts. Finally,\n",
    "    predictions are unscaled, scores are printed, and results are plotted and\n",
    "    saved.\"\"\"\n",
    "    \n",
    "    # Split into X & y and scale data:\n",
    "    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data,\n",
    "                                                                 test_data)\n",
    "\n",
    "    # Run sklearn models:\n",
    "    mod = model\n",
    "    mod.fit(X_train, y_train)\n",
    "    predictions = mod.predict(X_test) # y_pred=predictions\n",
    "\n",
    "    # Undo scaling to compare predictions against original data:\n",
    "    origin_df = m_df\n",
    "    unscaled = re_scaling(predictions, X_test, scaler_object) # unscaled_predictions\n",
    "    unscaled_df = prediction_df(unscaled, origin_df)\n",
    "\n",
    "    # Print scores and plot results:\n",
    "    get_scores(unscaled_df, origin_df, model_name)\n",
    "    plot_results(unscaled_df, origin_df, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**\n",
    "# Regressive Models\n",
    "* Linear Regression\n",
    "* Random Forest Regressor\n",
    "* XGBoost\n",
    "* LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.35425Z",
     "iopub.status.busy": "2022-09-28T19:11:41.353894Z",
     "iopub.status.idle": "2022-09-28T19:11:41.805471Z",
     "shell.execute_reply": "2022-09-28T19:11:41.804545Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.354219Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressive_model(train, test, LinearRegression(), 'LinearRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressive_model(train, test, Ridge(alpha=1.0), 'Ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressive_model(train, test, Lasso(alpha=0.1), 'Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressive_model(train, test, ElasticNet(alpha=0.1, l1_ratio=0.7), 'Elastic Net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressive_model(train, test, SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1), 'SupportVectorRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressive_model(train, test, DecisionTreeRegressor(max_depth=3), 'DecisionTree')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.807543Z",
     "iopub.status.busy": "2022-09-28T19:11:41.806919Z",
     "iopub.status.idle": "2022-09-28T19:11:42.34584Z",
     "shell.execute_reply": "2022-09-28T19:11:42.344801Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.80751Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressive_model(train, test, RandomForestRegressor(n_estimators=100, max_depth=20), \n",
    "          'RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressive_model(train, test, GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3), 'GradientBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressive_model(train, test, cbt.CatBoostRegressor(iterations=100, learning_rate=0.2, depth=3, silent=True), 'CatBoost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:42.348472Z",
     "iopub.status.busy": "2022-09-28T19:11:42.347615Z",
     "iopub.status.idle": "2022-09-28T19:11:43.113666Z",
     "shell.execute_reply": "2022-09-28T19:11:43.11238Z",
     "shell.execute_reply.started": "2022-09-28T19:11:42.348434Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regressive_model(train, test, XGBRegressor(n_estimators=100,max_depth=3, \n",
    "                                           learning_rate=0.2,objective='reg:squarederror'), 'XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
    "\n",
    "> LSTM is a type of recurring neural network that is especially useful for predicting sequential data. [Getting started with the Keras Sequential model](https://faroit.com/keras-docs/1.2.0/getting-started/sequential-model-guide/#sequence-classification-with-lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "def lstm_model(train_data, test_data):\n",
    "    \"\"\"Runs a long-short-term-memory neural net with 2 dense layers. \n",
    "    Generates predictions that are then unscaled. \n",
    "    Scores are printed and the results are plotted and saved.\"\"\"\n",
    "    # Assuming scale_data, re_scaling, prediction_df, get_scores, plot_results are defined elsewhere\n",
    "\n",
    "    # Split into X & y and scale data:\n",
    "    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "   \n",
    "    # Build LSTM:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(X_train.shape[1], X_train.shape[2]), stateful=False))  # Stateful is set to False for simplicity\n",
    "    model.add(Dense(1))  # Output layer\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=2, shuffle=False)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = model.predict(X_test, batch_size=1)\n",
    "\n",
    "    # Set the origin_df for comparison (ensure this is set appropriately)\n",
    "    origin_df = m_df  # or whatever data frame you are comparing to\n",
    "\n",
    "    # Undo scaling to compare predictions against original data:\n",
    "    unscaled = re_scaling(predictions, X_test, scaler_object, lstm=True)\n",
    "    unscaled_df = prediction_df(unscaled, origin_df)\n",
    "\n",
    "    # Get scores and plot results\n",
    "    get_scores(unscaled_df, origin_df, 'LSTM')\n",
    "    plot_results(unscaled_df, origin_df, 'LSTM')\n",
    "\n",
    "    return unscaled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:43.129098Z",
     "iopub.status.busy": "2022-09-28T19:11:43.128776Z",
     "iopub.status.idle": "2022-09-28T19:11:49.665515Z",
     "shell.execute_reply": "2022-09-28T19:11:49.664346Z",
     "shell.execute_reply.started": "2022-09-28T19:11:43.12907Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_model(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:49.667068Z",
     "iopub.status.busy": "2022-09-28T19:11:49.666757Z",
     "iopub.status.idle": "2022-09-28T19:11:49.673125Z",
     "shell.execute_reply": "2022-09-28T19:11:49.671939Z",
     "shell.execute_reply.started": "2022-09-28T19:11:49.667037Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_scores, open( \"model_scores.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling\n",
    "\n",
    "[ **SARIMAX Modeling**  ](https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/)\n",
    "\n",
    "> We use the statsmodels SARIMAX package to train the model and generate dynamic predictions. The SARIMA model breaks down into a few parts.\n",
    "\n",
    "     AR: represented as p, is the autoregressive model\n",
    "     I : represented as d, is the differencing term\n",
    "     MA: represented as q, is the moving average model\n",
    "     S: enables us to add a seasonal component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:49.675389Z",
     "iopub.status.busy": "2022-09-28T19:11:49.675043Z",
     "iopub.status.idle": "2022-09-28T19:11:49.685938Z",
     "shell.execute_reply": "2022-09-28T19:11:49.684531Z",
     "shell.execute_reply.started": "2022-09-28T19:11:49.675358Z"
    }
   },
   "outputs": [],
   "source": [
    "datatime_df.index = pd.to_datetime(datatime_df.index)\n",
    "datatime_df_t = datatime_df.copy()\n",
    "print(datatime_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <p style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ In the code below, we define our model and then make dynamic predictions for the last 12 months of the data. For standard, non-dynamic predictions, the following monthâ€™s prediction is made using the actual sales from the prior months. In contrast, for dynamic predictions, the following monthâ€™s prediction is made using the predicted sales from the prior months.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:39:01.682986Z",
     "iopub.status.busy": "2022-09-28T19:39:01.681763Z",
     "iopub.status.idle": "2022-09-28T19:39:01.693573Z",
     "shell.execute_reply": "2022-09-28T19:39:01.691874Z",
     "shell.execute_reply.started": "2022-09-28T19:39:01.682941Z"
    }
   },
   "outputs": [],
   "source": [
    "def sarimax_model(data):\n",
    "    # Model:\n",
    "    sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(12, 0, 0),\n",
    "                                    seasonal_order=(0, 1, 0, 12),\n",
    "                                    trend='c').fit()\n",
    "    \n",
    "    # Generate predictions:\n",
    "    start, end, dynamic = 40, 100, 7\n",
    "    data['pred_value'] = sar.predict(start=start, end=end, dynamic=dynamic)\n",
    "    pred_df = data.pred_value[start+dynamic:end]\n",
    "    \n",
    "    data[[\"sales_diff\",\"pred_value\"]].plot(color=['blue', 'Red'])\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    model_score = {}\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(data.sales_diff[-12:], data.pred_value[-12:]))\n",
    "    mae = mean_absolute_error(data.sales_diff[-12:], data.pred_value[-12:])\n",
    "    r2 = r2_score(data.sales_diff[-12:], data.pred_value[-12:])\n",
    "    model_scores['ARIMA'] = [rmse, mae, r2]\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\\nMAE: {mae}\\nR2 Score: {r2}\")\n",
    "    \n",
    "    return sar, data, pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:38:39.621394Z",
     "iopub.status.busy": "2022-09-28T19:38:39.62095Z",
     "iopub.status.idle": "2022-09-28T19:38:39.675106Z",
     "shell.execute_reply": "2022-09-28T19:38:39.67338Z",
     "shell.execute_reply.started": "2022-09-28T19:38:39.62136Z"
    }
   },
   "outputs": [],
   "source": [
    "sar, datatime_df, predictions = sarimax_model(datatime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.plot_diagnostics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:51.028531Z",
     "iopub.status.busy": "2022-09-28T19:11:51.028095Z",
     "iopub.status.idle": "2022-09-28T19:11:52.006526Z",
     "shell.execute_reply": "2022-09-28T19:11:52.005187Z",
     "shell.execute_reply.started": "2022-09-28T19:11:51.028496Z"
    }
   },
   "outputs": [],
   "source": [
    "sar.plot_diagnostics(figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.008493Z",
     "iopub.status.busy": "2022-09-28T19:11:52.008102Z",
     "iopub.status.idle": "2022-09-28T19:11:52.014205Z",
     "shell.execute_reply": "2022-09-28T19:11:52.012645Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.008452Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_scores, open( \"ARIMAmodel_scores.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatime_df_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example for Holt's Linear Trend Model\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "\n",
    "train_size = int(len(datatime_df_t) * 0.8)\n",
    "train_data, test_data = datatime_df_t['sales_diff'].iloc[:train_size], datatime_df_t['sales_diff'].iloc[train_size:]\n",
    "\n",
    "model = Holt(train_data)\n",
    "model_fit = model.fit(optimized=True)\n",
    "forecasts = model_fit.forecast(len(test_data))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(datatime_df_t['sales_diff'], label='Actual Sales', color='blue')\n",
    "plt.plot(train_data.index, model_fit.fittedvalues, label='Fitted by Holt', color='green')\n",
    "plt.plot(test_data.index, forecasts, label='Forecast by Holt', color='red')\n",
    "plt.title('Holtâ€™s Linear Trend Model Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparing Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.016452Z",
     "iopub.status.busy": "2022-09-28T19:11:52.016003Z",
     "iopub.status.idle": "2022-09-28T19:11:52.027589Z",
     "shell.execute_reply": "2022-09-28T19:11:52.026367Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.016392Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_results_df():\n",
    "    results_dict = pickle.load(open(\"model_scores.p\", \"rb\"))\n",
    "    \n",
    "    results_dict.update(pickle.load(open(\"ARIMAmodel_scores.p\", \"rb\")))\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient='index', \n",
    "                                        columns=['RMSE', 'MAE','R2'])\n",
    "    \n",
    "    results_df = results_df.sort_values(by='RMSE', ascending=False).reset_index()\n",
    "    \n",
    "    results_df.to_csv('./results.csv')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    sns.lineplot(np.arange(len(results_df)), 'RMSE', data=results_df, ax=ax, \n",
    "                 label='RMSE', color='darkblue')\n",
    "    sns.lineplot(np.arange(len(results_df)), 'MAE', data=results_df, ax=ax, \n",
    "                 label='MAE', color='Cyan')\n",
    "    \n",
    "    plt.xticks(np.arange(len(results_df)),rotation=45)\n",
    "    ax.set_xticklabels(results_df['index'])\n",
    "    ax.set(xlabel = \"Model\",\n",
    "           ylabel = \"Scores\",\n",
    "           title = \"Model Error Comparison\")\n",
    "    sns.despine()\n",
    "    \n",
    "    plt.savefig(f'./model_output/compare_models.png')\n",
    "    \n",
    "    return results_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_results_df():\n",
    "    # Assuming results_dict is already loaded properly\n",
    "    results_dict = pickle.load(open(\"model_scores.p\", \"rb\"))\n",
    "    results_dict.update(pickle.load(open(\"ARIMAmodel_scores.p\", \"rb\")))\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient='index', columns=['RMSE', 'MAE', 'R2'])\n",
    "    results_df.to_csv('./results.csv')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    sns.lineplot(x=np.arange(len(results_df)), y='RMSE', data=results_df, ax=ax, label='RMSE', color='darkblue')\n",
    "    sns.lineplot(x=np.arange(len(results_df)), y='MAE', data=results_df, ax=ax, label='MAE', color='Cyan')\n",
    "\n",
    "    plt.xticks(np.arange(len(results_df)), results_df.index, rotation=45)\n",
    "    plt.title(\"Performance Metrics Comparison\")\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.03017Z",
     "iopub.status.busy": "2022-09-28T19:11:52.029074Z",
     "iopub.status.idle": "2022-09-28T19:11:52.397584Z",
     "shell.execute_reply": "2022-09-28T19:11:52.396285Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.030124Z"
    }
   },
   "outputs": [],
   "source": [
    "results = create_results_df()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.399598Z",
     "iopub.status.busy": "2022-09-28T19:11:52.398881Z",
     "iopub.status.idle": "2022-09-28T19:11:52.405747Z",
     "shell.execute_reply": "2022-09-28T19:11:52.404747Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.399563Z"
    }
   },
   "outputs": [],
   "source": [
    "avarage_12months()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.407405Z",
     "iopub.status.busy": "2022-09-28T19:11:52.407057Z",
     "iopub.status.idle": "2022-09-28T19:11:52.416874Z",
     "shell.execute_reply": "2022-09-28T19:11:52.415835Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.40737Z"
    }
   },
   "outputs": [],
   "source": [
    "average = 894478.3333333334\n",
    "XGBoost = results.MAE.values[4]\n",
    "percentage_off = round(XGBoost/average*100,2)\n",
    "\n",
    "print(f\"With XGBoost, prediction is within {percentage_off}% of the actual.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868225,
     "sourceId": 9999,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30235,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
