{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d6235ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "import matplotlib.colors as col\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path  \n",
    "import random\n",
    "\n",
    "# Scikit-Learn models:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import catboost as cbt\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# LSTM:\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# ARIMA Model:\n",
    "\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "df_s = pd.read_excel(\"data/Argent_AFO_46045_SS_ship.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158c9b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 1/11/2024 1:43:40 PM\n",
      "max 9/8/2023 7:36:06 AM\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_name):\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "df_s = df_s[[\"Created\", \"Sku\", \"AdjustmentQty\"]]\n",
    "df_s.rename(columns={\"Created\": \"Date\", \"AdjustmentQty\": \"ShippedQty\"}, inplace=True)\n",
    "\n",
    "\n",
    "print(\"min\", df_s[\"Date\"].min())\n",
    "print(\"max\", df_s[\"Date\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88e9c365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date           Sku  ShippedQty\n",
      "0      4/27/2023 6:58:37 AM  AFO 46045 SS           1\n",
      "1      4/27/2023 7:10:36 AM  AFO 46045 SS           1\n",
      "2      4/27/2023 7:10:36 AM  AFO 46045 SS           1\n",
      "3      4/28/2023 9:30:39 AM  AFO 46045 SS           1\n",
      "4      5/1/2023 10:47:34 AM  AFO 46045 SS           1\n",
      "...                     ...           ...         ...\n",
      "11082  11/6/2023 7:17:41 AM  AFO 46045 SS           1\n",
      "11083  11/6/2023 7:17:41 AM  AFO 46045 SS           1\n",
      "11084  11/6/2023 7:17:41 AM  AFO 46045 SS           1\n",
      "11085  11/6/2023 7:17:41 AM  AFO 46045 SS           1\n",
      "11086  11/6/2023 7:17:41 AM  AFO 46045 SS           1\n",
      "\n",
      "[11087 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067030a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_data = load_data('data/train.csv')\n",
    "# df_s = sales_data.copy()\n",
    "# df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7dca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = load_data('data/test.csv')\n",
    "# df_t = test_data.copy()\n",
    "# df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae57421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"min\", df_s[\"date\"].min())\n",
    "# print(\"max\",dsf df_t[\"date\"].max())\n",
    "# # print(913000/45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_aggregation(data, time=\"monthly\"):\n",
    "    data = data.copy()\n",
    "    \n",
    "    if time == \"monthly\":\n",
    "        data[\"date\"] = pd.to_datetime(data[\"date\"]).dt.to_period(\"M\").astype(str)\n",
    "    elif time == \"yearly\":\n",
    "        data[\"date\"] = pd.to_datetime(data[\"date\"]).dt.to_period(\"Y\").astype(str)\n",
    "    elif time == \"weekly\":\n",
    "        data[\"date\"] = pd.to_datetime(data[\"date\"]).dt.to_period(\"W\").astype(str)\n",
    "    elif time == \"daily\":\n",
    "        data[\"date\"] = pd.to_datetime(data[\"date\"]).dt.date  # Keeps full date\n",
    "    else:\n",
    "        raise ValueError(\"Invalid time period. Choose from 'daily', 'weekly', 'monthly', or 'yearly'.\")\n",
    "    \n",
    "    # Sum sales per selected time period\n",
    "    data = data.groupby(\"date\")[\"sales\"].sum().reset_index()\n",
    "    \n",
    "    # Convert date column back to datetime format (except for weekly)\n",
    "    if time in [\"monthly\", \"yearly\", \"daily\"]:\n",
    "        data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "m_df = sales_aggregation(df_s, \"monthly\")\n",
    "y_df = sales_aggregation(df_s, \"yearly\")\n",
    "w_df = sales_aggregation(df_s, \"weekly\")\n",
    "d_df = sales_aggregation(df_s, \"daily\")\n",
    "\n",
    "\n",
    "layout = (2, 2)\n",
    "\n",
    "plot1 = plt.subplot2grid(layout, (0 ,0))\n",
    "plot2 = plt.subplot2grid(layout, (0 ,1))\n",
    "plot3 = plt.subplot2grid(layout, (1 ,0))\n",
    "plot4 = plt.subplot2grid(layout, (1 ,1))\n",
    "\n",
    "years = y_df['sales'].plot(kind = \"bar\",color = 'mediumblue', label=\"Sales\",ax=plot1, figsize=(12,5))\n",
    "months = m_df['sales'].plot(kind = \"bar\",color = 'mediumblue', label=\"Sales\",ax=plot2, figsize=(12,5))\n",
    "# months = m_df['sales'].plot(marker = 'o',color = 'darkorange', label=\"Sales\", ax=plot2)\n",
    "weeks = w_df['sales'].plot(marker = 'o',color = 'darkorange', label=\"Sales\", ax=plot3)\n",
    "days = d_df['sales'].plot(color = 'darkorange', label=\"Sales\", ax=plot4)\n",
    "\n",
    "years.set(xlabel = \"Years\",title = \"Distribution of Sales Per Year\")\n",
    "months.set(xlabel = \"Months\", title = \"Distribution of Sales Per Month\")\n",
    "weeks.set(xlabel = \"Weeks\", title = \"Distribution of Sales Per Week\")\n",
    "days.set(xlabel = \"Days\", title = \"Distribution of Sales Per Day\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "years.legend()\n",
    "months.legend()\n",
    "weeks.legend()\n",
    "days.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7890a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce379c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_plot(data, x_col, y_col, title):\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    sns.lineplot(x=x_col, y=y_col, data=data, ax=ax, color='darkblue', label='Total Sales')\n",
    "    \n",
    "    # Calculating the yearly mean of sales\n",
    "    s_mean = data.groupby(data[x_col].dt.year)[y_col].mean().reset_index()\n",
    "    s_mean[x_col] = pd.to_datetime(s_mean[x_col], format='%Y')  # Adjust this if the format doesn't match\n",
    "\n",
    "    # Plotting the yearly mean\n",
    "    sns.lineplot(x=s_mean[x_col], y=s_mean[y_col], ax=ax, color='darkred', label='Yearly Average Sales')\n",
    "\n",
    "    ax.set(title=title, xlabel=x_col, ylabel=y_col)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def get_diff(data):\n",
    "    \"\"\"Calculate the difference in sales month over month:\"\"\"\n",
    "    \n",
    "    data['sales_diff'] = data.sales.diff()\n",
    "    data = data.dropna()\n",
    "    \n",
    "    data.to_csv('./stationary_df.csv')\n",
    "    \n",
    "    return data\n",
    "    \n",
    "time_plot(m_df, 'date', 'sales', 'Distribution of Sales Per Mounth' )\n",
    "stationary_df = get_diff(m_df)\n",
    "time_plot(stationary_df, 'date', 'sales_diff', \n",
    "          'Monthly Sales After Diff Transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124fb33e",
   "metadata": {},
   "source": [
    "### ARIMA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_arima_data(data):\n",
    "    \n",
    "#     da_data = data.set_index('date').drop('sales', axis=1)\n",
    "#     da_data.dropna(axis=0)\n",
    "#     da_data.to_csv('./arima_df.csv')    \n",
    "#     return da_data\n",
    "\n",
    "# datatime_df = build_arima_data(stationary_df)\n",
    "\n",
    "# def plots_lag(data, lags=None):\n",
    "#     \"\"\"Convert dataframe to datetime index\"\"\"\n",
    "#     dt_data = data.set_index('date').drop('sales', axis=1)\n",
    "#     dt_data.dropna(axis=0)\n",
    "    \n",
    "    \n",
    "#     law  = plt.subplot(122)\n",
    "#     acf  = plt.subplot(221)\n",
    "#     pacf = plt.subplot(223)\n",
    "    \n",
    "#     dt_data.plot(ax=law, figsize=(10, 5), color='orange')\n",
    "#     # Plot the autocorrelation function:\n",
    "#     smt.graphics.plot_acf(dt_data, lags=lags, ax=acf, color='mediumblue')\n",
    "#     smt.graphics.plot_pacf(dt_data, lags=lags, ax=pacf, color='mediumblue')\n",
    "    \n",
    "#     # Will also adjust spacing between subplots to minimize the overlaps:\n",
    "#     plt.tight_layout()\n",
    "\n",
    "# plots_lag(stationary_df, lags=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26451ed6",
   "metadata": {},
   "source": [
    "### Regressive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f18b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_df = stationary_df\n",
    "print(model_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52b821",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_df.columns)\n",
    "print(model_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd86859",
   "metadata": {},
   "outputs": [],
   "source": [
    "edwfrgethry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_and_scale(data, train_size=0.8):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and test sets, removes 'sales' and 'date' columns,\n",
    "    and scales the data using MinMaxScaler.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): Input dataset with a target column.\n",
    "        train_size (float): Proportion of data to be used for training (default is 0.8).\n",
    "    \n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test, scaler: Scaled train and test sets with the scaler object.\n",
    "    \"\"\"\n",
    "    # Ensure 'sales' and 'date' exist before dropping\n",
    "    columns_to_drop = [col for col in ['sales', 'date'] if col in data.columns]\n",
    "    data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Ensure data has at least 2 columns (features + target)\n",
    "    if data.shape[1] < 2:\n",
    "        raise ValueError(\"Dataset must have at least 2 columns (features + target).\")\n",
    "\n",
    "    # Train-test split\n",
    "    train_index = int(len(data) * train_size)  # 80% train, 20% test\n",
    "    train_set, test_set = data.iloc[:train_index].values, data.iloc[train_index:].values\n",
    "\n",
    "    # Apply MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(train_set)  # Fit only on training data\n",
    "\n",
    "    # Scale datasets\n",
    "    train_set_scaled = scaler.transform(train_set)\n",
    "    test_set_scaled = scaler.transform(test_set)\n",
    "\n",
    "    # Feature-target split\n",
    "    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0].ravel()\n",
    "    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0].ravel()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "# Example usage\n",
    "X_train, y_train, X_test, y_test, scaler_object = train_test_split_and_scale(model_df)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Shape of X Train: {X_train.shape}\")\n",
    "print(f\"Shape of y Train: {y_train.shape}\")\n",
    "print(f\"Shape of X Test: {X_test.shape}\")\n",
    "print(f\"Shape of y Test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae7611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_test_split(data, train_size=0.8):\n",
    "    data = data.drop(['sales', 'date'], axis=1)\n",
    "    \n",
    "    train_index = int(len(data) * train_size)  # 80% for training\n",
    "    train, test = data[:train_index].values, data[train_index:].values\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(model_df)\n",
    "print(f\"Shape of Train: {train.shape}\\nShape of Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57e908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01374a5",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb63534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train_set,test_set):\n",
    "\n",
    "    \n",
    "    # Apply Min Max Scaler:\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train_set)\n",
    "    \n",
    "    # Reshape training set:\n",
    "    train_set = train_set.reshape(train_set.shape[0],\n",
    "                                  train_set.shape[1])\n",
    "    train_set_scaled = scaler.transform(train_set)\n",
    "    \n",
    "    # Reshape test set:\n",
    "    test_set = test_set.reshape(test_set.shape[0], \n",
    "                                test_set.shape[1])\n",
    "    test_set_scaled = scaler.transform(test_set)\n",
    "    \n",
    "    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel() # returns the array, flattened!\n",
    "    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_object = scale_data(train, test)\n",
    "print(f\"Shape of X Train: {X_train.shape}\\nShape of y Train: {y_train.shape}\\nShape of X Test: {X_test.shape}\\nShape of y Test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6eb90f",
   "metadata": {},
   "source": [
    "### Reverse Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_scaling(y_pred, x_test, scaler_obj, lstm=False):\n",
    "    \"\"\"For visualizing and comparing results, undoes the scaling effect on predictions.\"\"\"\n",
    "   # y_pred: model predictions\n",
    "   # x_test: features from the test set used for predictions\n",
    "   # scaler_obj: the scaler objects used for min-max scaling\n",
    "   # lstm: indicate if the model run is the lstm. If True, additional transformation occurs \n",
    "    \n",
    "    # Reshape y_pred:\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0],\n",
    "                            1,\n",
    "                            1)\n",
    "\n",
    "    if not lstm:\n",
    "        x_test = x_test.reshape(x_test.shape[0],\n",
    "                                1, \n",
    "                                x_test.shape[1])\n",
    "\n",
    "    # Rebuild test set for inverse transform:\n",
    "    pred_test_set = []\n",
    "    for index in range(0, len(y_pred)):\n",
    "        pred_test_set.append(np.concatenate([y_pred[index], \n",
    "                                             x_test[index]],\n",
    "                                             axis=1) )\n",
    "\n",
    "    # Reshape pred_test_set:\n",
    "    pred_test_set = np.array(pred_test_set)\n",
    "    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0],\n",
    "                                          pred_test_set.shape[2])\n",
    "\n",
    "    # Inverse transform:\n",
    "    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)\n",
    "\n",
    "    return pred_test_set_inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_df(unscale_predictions, origin_df):\n",
    "    \"\"\"Generates a dataframe that shows the predicted sales for each month\n",
    "    for plotting results.\"\"\"\n",
    "    \n",
    "    # unscale_predictions: the model predictions that do not have min-max or other scaling applied\n",
    "    # origin_df: the original monthly sales dataframe\n",
    "    \n",
    "    # Create dataframe that shows the predicted sales:\n",
    "    result_list = []\n",
    "    sales_dates = list(origin_df[-13:].date)\n",
    "    act_sales = list(origin_df[-13:].sales)\n",
    "\n",
    "    for index in range(0, len(unscale_predictions)):\n",
    "        result_dict = {}\n",
    "        result_dict['pred_value'] = int(unscale_predictions[index][0] + act_sales[index])\n",
    "        result_dict['date'] = sales_dates[index + 1]\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f14b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "\n",
    "def get_scores(unscale_df, origin_df, model_name):\n",
    "    \"\"\"Prints the root mean squared error, mean absolute error, and r2 scores\n",
    "    for each model. Saves all results in a model_scores dictionary for\n",
    "    comparison.\"\"\"\n",
    "\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(origin_df.sales[-12:], \n",
    "                                      unscale_df.pred_value[-12:]))\n",
    "    \n",
    "    mae = mean_absolute_error(origin_df.sales[-12:], \n",
    "                              unscale_df.pred_value[-12:])\n",
    "    \n",
    "    r2 = r2_score(origin_df.sales[-12:], \n",
    "                  unscale_df.pred_value[-12:])\n",
    "    \n",
    "    model_scores[model_name] = [rmse, mae, r2]\n",
    "\n",
    "    print(f\"RMSE: {rmse}\\nMAE: {mae}\\nR2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbeb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bcc3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, origin_df, model_name):\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    sns.lineplot(x='date', y='sales', data=origin_df, ax=ax, label='Original', color='blue')\n",
    "    sns.lineplot(x='date', y='pred_value', data=results, ax=ax, label='Predicted', color='red')\n",
    "    \n",
    "    ax.set(xlabel=\"Date\", ylabel=\"Sales\", title=f\"{model_name} Sales Forecasting Prediction\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    filepath = Path('./model_output/{model_name}_forecasting.svg')  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True) \n",
    "    plt.savefig(f'./model_output/{model_name}_forecasting.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressive_model(train_data, test_data, model, model_name):\n",
    "    \"\"\"Runs regressive models in SKlearn framework. First calls scale_data\n",
    "    to split into X and y and scale the data. Then fits and predicts. Finally,\n",
    "    predictions are unscaled, scores are printed, and results are plotted and\n",
    "    saved.\"\"\"\n",
    "    \n",
    "    # Split into X & y and scale data:\n",
    "    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data,\n",
    "                                                                 test_data)\n",
    "\n",
    "    # Run sklearn models:\n",
    "    mod = model\n",
    "    mod.fit(X_train, y_train)\n",
    "    predictions = mod.predict(X_test) # y_pred=predictions\n",
    "\n",
    "    # Undo scaling to compare predictions against original data:\n",
    "    origin_df = m_df\n",
    "    unscaled = re_scaling(predictions, X_test, scaler_object) # unscaled_predictions\n",
    "    unscaled_df = prediction_df(unscaled, origin_df)\n",
    "\n",
    "    # Print scores and plot results:\n",
    "    get_scores(unscaled_df, origin_df, model_name)\n",
    "    plot_results(unscaled_df, origin_df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eda416",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressive_model(train, test, LinearRegression(), 'LinearRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3773470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b293f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce1b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e458008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77112d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51049010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
