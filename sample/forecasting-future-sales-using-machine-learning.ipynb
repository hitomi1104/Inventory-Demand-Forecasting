{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9999,"databundleVersionId":868225,"sourceType":"competition"}],"dockerImageVersionId":30235,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Forecasting Future Sales Using Machine Learning\n\nForecasting future sales of a product offers many advantages. Predicting future sales of a product helps a company manage the cost of manufacturing and marketing the product. In this notebook, I will try to you through the task of future sales prediction with machine learning using Python.","metadata":{}},{"cell_type":"code","source":"# EDA Libraries:\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.colors as col\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nfrom pathlib import Path  \nimport random\n\n# Scikit-Learn models:\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\n# LSTM:\n\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom keras.layers import LSTM\n\n\n# ARIMA Model:\n\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nfrom statsmodels.tools.eval_measures import rmse\n\n\nimport pickle\nimport warnings\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:28.333232Z","iopub.execute_input":"2022-09-28T19:11:28.334615Z","iopub.status.idle":"2022-09-28T19:11:36.257768Z","shell.execute_reply.started":"2022-09-28T19:11:28.334479Z","shell.execute_reply":"2022-09-28T19:11:36.256584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:36.259813Z","iopub.execute_input":"2022-09-28T19:11:36.260548Z","iopub.status.idle":"2022-09-28T19:11:36.265685Z","shell.execute_reply.started":"2022-09-28T19:11:36.260514Z","shell.execute_reply":"2022-09-28T19:11:36.264462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Loading Dataset & Data Exploration (EDA)** ","metadata":{}},{"cell_type":"markdown","source":"   The first step is to load the data and transform it into a structure that we will then use for each of our models. In its raw form, each row of data represents a single day of sales at one of ten stores. Our goal is to predict monthly sales, so we will first consolidate all stores and days into total monthly sales.","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/demand-forecasting-kernels-only/sample_submission.csv')\ndf = dataset.copy()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:36.26715Z","iopub.execute_input":"2022-09-28T19:11:36.267634Z","iopub.status.idle":"2022-09-28T19:11:36.33136Z","shell.execute_reply.started":"2022-09-28T19:11:36.267578Z","shell.execute_reply":"2022-09-28T19:11:36.330112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(file_name):\n    \"\"\"Returns a pandas dataframe from a csv file.\"\"\"\n    return pd.read_csv(file_name)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:36.334422Z","iopub.execute_input":"2022-09-28T19:11:36.334911Z","iopub.status.idle":"2022-09-28T19:11:36.340923Z","shell.execute_reply.started":"2022-09-28T19:11:36.334865Z","shell.execute_reply":"2022-09-28T19:11:36.339757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_data = load_data('../input/demand-forecasting-kernels-only/train.csv')\ndf_s = sales_data.copy()\ndf_s.info()","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:36.34287Z","iopub.execute_input":"2022-09-28T19:11:36.343382Z","iopub.status.idle":"2022-09-28T19:11:37.050788Z","shell.execute_reply.started":"2022-09-28T19:11:36.343331Z","shell.execute_reply":"2022-09-28T19:11:37.04965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_s.tail()","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:37.052373Z","iopub.execute_input":"2022-09-28T19:11:37.052694Z","iopub.status.idle":"2022-09-28T19:11:37.06468Z","shell.execute_reply.started":"2022-09-28T19:11:37.052666Z","shell.execute_reply":"2022-09-28T19:11:37.063493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To view basic statistical details about dataset:\n\ndf_s['sales'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:37.066191Z","iopub.execute_input":"2022-09-28T19:11:37.066938Z","iopub.status.idle":"2022-09-28T19:11:37.117364Z","shell.execute_reply.started":"2022-09-28T19:11:37.066895Z","shell.execute_reply":"2022-09-28T19:11:37.116195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n    <h4 style=\"padding: 15px;\n              color:black;\">ðŸ“Œ Sales seem to be unbalanced!\n    </h4>\n  </div>","metadata":{}},{"cell_type":"code","source":"df_s['sales'].plot()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:37.1196Z","iopub.execute_input":"2022-09-28T19:11:37.120067Z","iopub.status.idle":"2022-09-28T19:11:37.650172Z","shell.execute_reply.started":"2022-09-28T19:11:37.120012Z","shell.execute_reply":"2022-09-28T19:11:37.649108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Returns a dataframe where each row represents total sales for a given month. Columns include 'date' by month and 'sales'.\n    ","metadata":{}},{"cell_type":"code","source":"def monthlyORyears_sales(data,time=['monthly','years']):\n    data = data.copy()\n    if time == \"monthly\":\n        # Drop the day indicator from the date column:\n        data.date = data.date.apply(lambda x: str(x)[:-3])\n    else:\n        data.date = data.date.apply(lambda x: str(x)[:4])\n        \n   # Sum sales per month: \n    data = data.groupby('date')['sales'].sum().reset_index()\n    data.date = pd.to_datetime(data.date)\n        \n    return data\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:37.651552Z","iopub.execute_input":"2022-09-28T19:11:37.651889Z","iopub.status.idle":"2022-09-28T19:11:37.659058Z","shell.execute_reply.started":"2022-09-28T19:11:37.651858Z","shell.execute_reply":"2022-09-28T19:11:37.657963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_df = monthlyORyears_sales(df_s,\"monthly\")\n\nm_df.to_csv('./monthly_data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:37.662607Z","iopub.execute_input":"2022-09-28T19:11:37.662972Z","iopub.status.idle":"2022-09-28T19:11:38.171218Z","shell.execute_reply.started":"2022-09-28T19:11:37.662928Z","shell.execute_reply":"2022-09-28T19:11:38.170417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:38.172628Z","iopub.execute_input":"2022-09-28T19:11:38.173179Z","iopub.status.idle":"2022-09-28T19:11:38.183395Z","shell.execute_reply.started":"2022-09-28T19:11:38.173145Z","shell.execute_reply":"2022-09-28T19:11:38.182255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> In our new data frame, each row now represents the total sales for a given month across stores.","metadata":{}},{"cell_type":"code","source":"y_df = monthlyORyears_sales(df_s,\"years\")\ny_df","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:38.184519Z","iopub.execute_input":"2022-09-28T19:11:38.184808Z","iopub.status.idle":"2022-09-28T19:11:38.676214Z","shell.execute_reply.started":"2022-09-28T19:11:38.184782Z","shell.execute_reply":"2022-09-28T19:11:38.675384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layout = (1, 2)\n\nraw = plt.subplot2grid(layout, (0 ,0))\nlaw = plt.subplot2grid(layout, (0 ,1))\n\nyears = y_df['sales'].plot(kind = \"bar\",color = 'mediumblue', label=\"Sales\",ax=raw, figsize=(12,5))\nmonths = m_df['sales'].plot(marker = 'o',color = 'darkorange', label=\"Sales\", ax=law)\n\nyears.set(xlabel = \"Years\",title = \"Distribution of Sales Per Year\")\nmonths.set(xlabel = \"Months\", title = \"Distribution of Sales Per Mounth\")\n\nsns.despine()\nplt.tight_layout()\n\nyears.legend()\nmonths.legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:38.67722Z","iopub.execute_input":"2022-09-28T19:11:38.67754Z","iopub.status.idle":"2022-09-28T19:11:39.19411Z","shell.execute_reply.started":"2022-09-28T19:11:38.677512Z","shell.execute_reply":"2022-09-28T19:11:39.192798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n    <p style=\"padding: 15px;\n              color:black;\">ðŸ“Œ If we plot the total monthly sales over time, we see that the average monthly sales increase over time, so our data is not stationary.\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"[ðŸ¦Ž TREND AND SEASONALITY](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n> There are many other models to forecast time series, such as weighted\nmoving average models or autoregressive integrated moving average\n(ARIMA) models. Some of them require you to first remove the trend\nand seasonality. For example, if you are studying the number of active\nusers on your website, and it is growing by 10% every month, you\nwould have to remove this trend from the time series. Once the model\nis trained and starts making predictions, you would have to add the\ntrend back to get the final predictions. Similarly, if you are trying to\npredict the amount of sunscreen lotion sold every month, you will\nprobably observe strong seasonality: since it sells well every summer,\na similar pattern will be repeated every year. You would have to\nremove this seasonality from the time series, for example by\ncomputing the difference between the value at each time step and the\nvalue one year earlier (this technique is called differencing). Again,\nafter the model is trained and makes predictions, you would have to\nadd the seasonal pattern back to get the final predictions.\n","metadata":{}},{"cell_type":"markdown","source":"# **Data Exploration (EDA)**","metadata":{}},{"cell_type":"markdown","source":" To make it stationary, we will calculate the difference between the sales of each month and add it to our data frame as a new column.\n> Further details of the stationary and differences are available [here.](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/) But first let's take a closer look at the data set you work with by learning more about the data.","metadata":{}},{"cell_type":"code","source":"def sales_time(data):\n    \"\"\"Time interval of dataset:\"\"\"\n\n    data.date = pd.to_datetime(data.date)\n    n_of_days = data.date.max() - data.date.min()\n    n_of_years = int(n_of_days.days / 365)\n    \n    print(f\"Days: {n_of_days.days}\\nYears: {n_of_years}\\nMonth: {12 * n_of_years}\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:39.195555Z","iopub.execute_input":"2022-09-28T19:11:39.195933Z","iopub.status.idle":"2022-09-28T19:11:39.202363Z","shell.execute_reply.started":"2022-09-28T19:11:39.1959Z","shell.execute_reply":"2022-09-28T19:11:39.201059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_time(df_s)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:39.204548Z","iopub.execute_input":"2022-09-28T19:11:39.20501Z","iopub.status.idle":"2022-09-28T19:11:39.354247Z","shell.execute_reply.started":"2022-09-28T19:11:39.204968Z","shell.execute_reply":"2022-09-28T19:11:39.353037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sales Data Per Store**","metadata":{}},{"cell_type":"code","source":"# Let's sell it per store:\n\ndef sales_per_store(data):\n    sales_by_store = data.groupby('store')['sales'].sum().reset_index()\n    \n    fig, ax = plt.subplots(figsize=(8,6))\n    sns.barplot(sales_by_store.store, sales_by_store.sales, color='darkred')\n    \n    ax.set(xlabel = \"Store Id\", ylabel = \"Sum of Sales\", title = \"Total Sales Per Store\")\n    \n    return sales_by_store\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:39.355978Z","iopub.execute_input":"2022-09-28T19:11:39.356381Z","iopub.status.idle":"2022-09-28T19:11:39.364014Z","shell.execute_reply.started":"2022-09-28T19:11:39.356332Z","shell.execute_reply":"2022-09-28T19:11:39.363068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_per_store(df_s)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:39.36518Z","iopub.execute_input":"2022-09-28T19:11:39.36568Z","iopub.status.idle":"2022-09-28T19:11:39.664247Z","shell.execute_reply.started":"2022-09-28T19:11:39.365647Z","shell.execute_reply":"2022-09-28T19:11:39.663088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mean Monthly Sales**","metadata":{}},{"cell_type":"code","source":"# Overall for 5 years:\n\naverage_m_sales = m_df.sales.mean()\nprint(f\"Overall Avarage Monthly Sales: ${average_m_sales}\")\n\ndef avarage_12months():\n# Last 1 years (this will be the forecasted sales):\n    average_m_sales_1y = m_df.sales[-12:].mean()\n    print(f\"Last 12 months average monthly sales: ${average_m_sales_1y}\")\navarage_12months()","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:39.665827Z","iopub.execute_input":"2022-09-28T19:11:39.666317Z","iopub.status.idle":"2022-09-28T19:11:39.674328Z","shell.execute_reply.started":"2022-09-28T19:11:39.666247Z","shell.execute_reply":"2022-09-28T19:11:39.67322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Determining Time Series Stationary**","metadata":{}},{"cell_type":"markdown","source":"The underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented in this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back.","metadata":{}},{"cell_type":"code","source":"def time_plot(data, x_col, y_col, title):\n    fig, ax = plt.subplots(figsize = (15,8))\n    sns.lineplot(x_col, y_col, data = data, ax = ax, color = 'darkblue', label='Total Sales')\n    \n    s_mean = data.groupby(data.date.dt.year)[y_col].mean().reset_index()\n    s_mean.date = pd.to_datetime(s_mean.date, format='%Y')\n    sns.lineplot((s_mean.date + datetime.timedelta(6*365/12)), y_col, data=s_mean, ax=ax, color='red', label='Mean Sales')   \n    \n    ax.set(xlabel = \"Years\",\n           ylabel = \"Sales\",\n           title = title)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:39.67573Z","iopub.execute_input":"2022-09-28T19:11:39.676324Z","iopub.status.idle":"2022-09-28T19:11:39.689424Z","shell.execute_reply.started":"2022-09-28T19:11:39.676278Z","shell.execute_reply":"2022-09-28T19:11:39.688367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_plot(m_df, 'date', 'sales', 'Monthly Sales Before Diff Transformation' )","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:39.691161Z","iopub.execute_input":"2022-09-28T19:11:39.691874Z","iopub.status.idle":"2022-09-28T19:11:40.068119Z","shell.execute_reply.started":"2022-09-28T19:11:39.691831Z","shell.execute_reply":"2022-09-28T19:11:40.066881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Differencing**\n\nIn this method, we compute the difference of consecutive terms in the series. Differencing is typically performed to get rid of the varying mean. ","metadata":{}},{"cell_type":"code","source":"def get_diff(data):\n    \"\"\"Calculate the difference in sales month over month:\"\"\"\n    \n    data['sales_diff'] = data.sales.diff()\n    data = data.dropna()\n    \n    data.to_csv('./stationary_df.csv')\n    \n    return data\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:40.069482Z","iopub.execute_input":"2022-09-28T19:11:40.069828Z","iopub.status.idle":"2022-09-28T19:11:40.076522Z","shell.execute_reply.started":"2022-09-28T19:11:40.069799Z","shell.execute_reply":"2022-09-28T19:11:40.075224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stationary_df = get_diff(m_df)\ntime_plot(stationary_df, 'date', 'sales_diff', \n          'Monthly Sales After Diff Transformation')","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:40.078706Z","iopub.execute_input":"2022-09-28T19:11:40.079688Z","iopub.status.idle":"2022-09-28T19:11:40.447236Z","shell.execute_reply.started":"2022-09-28T19:11:40.079644Z","shell.execute_reply":"2022-09-28T19:11:40.446366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n    <h5 style=\"padding: 15px;\n              color:black;\">ðŸ“Œ Now that our data represent monthly sales and we have transformed it to be stationary, we will set up the \n                            data for our different model types. <br> <br>\n              âœ” To do this, we will \n                            define two different structures: \n        <br> <br>\n                    1. one will be used for ARIMA modeling,\n        <br><br>\n                    2. the other will be used for the rest of the models.\n    </h5>\n</div>\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Preparing Dataset Modeling**","metadata":{}},{"cell_type":"markdown","source":"# ARIMA Modeling\n\n>  For our Arima model, we will need only a datetime index and the dependent variable (diff in sales) columns.","metadata":{}},{"cell_type":"code","source":"def build_arima_data(data):\n    \"\"\"Generates a csv-file with a datetime index and a dependent sales column for ARIMA modeling.\"\"\"\n    \n    da_data = data.set_index('date').drop('sales', axis=1)\n    da_data.dropna(axis=0)\n    \n    da_data.to_csv('./arima_df.csv')\n    \n    return da_data\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:40.448314Z","iopub.execute_input":"2022-09-28T19:11:40.448968Z","iopub.status.idle":"2022-09-28T19:11:40.454539Z","shell.execute_reply.started":"2022-09-28T19:11:40.448935Z","shell.execute_reply":"2022-09-28T19:11:40.453661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datatime_df = build_arima_data(stationary_df)\ndatatime_df # ARIMA Dataframe","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:40.455979Z","iopub.execute_input":"2022-09-28T19:11:40.457149Z","iopub.status.idle":"2022-09-28T19:11:40.483662Z","shell.execute_reply.started":"2022-09-28T19:11:40.457065Z","shell.execute_reply":"2022-09-28T19:11:40.482544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Observing Lags**","metadata":{}},{"cell_type":"markdown","source":"For our other models, we will create a new data frame where each feature represents a previous monthâ€™s sales. To determine how many months to include in our feature set, we will observe the autocorrelation and partial autocorrelation plots and use the [rules for selecting lags in ARIMA modeling](https://people.duke.edu/~rnau/arimrule.htm). This way, we can keep consistent a look-back period for our ARIMA and regressive models.\n\n>  **[Statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html)**\n\n","metadata":{}},{"cell_type":"code","source":"def plots_lag(data, lags=None):\n    \"\"\"Convert dataframe to datetime index\"\"\"\n    dt_data = data.set_index('date').drop('sales', axis=1)\n    dt_data.dropna(axis=0)\n    \n    \n    law  = plt.subplot(122)\n    acf  = plt.subplot(221)\n    pacf = plt.subplot(223)\n    \n    dt_data.plot(ax=law, figsize=(10, 5), color='orange')\n    # Plot the autocorrelation function:\n    smt.graphics.plot_acf(dt_data, lags=lags, ax=acf, color='mediumblue')\n    smt.graphics.plot_pacf(dt_data, lags=lags, ax=pacf, color='mediumblue')\n    \n    # Will also adjust spacing between subplots to minimize the overlaps:\n    plt.tight_layout()\n\nplots_lag(stationary_df, lags=24);\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:40.48506Z","iopub.execute_input":"2022-09-28T19:11:40.485415Z","iopub.status.idle":"2022-09-28T19:11:41.117606Z","shell.execute_reply.started":"2022-09-28T19:11:40.485385Z","shell.execute_reply":"2022-09-28T19:11:41.116363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n> [ðŸŽ¦ Autocorrelation Function](https://www.youtube.com/watch?v=ZjaBn93YPWo&t=12s)\n\n\n>[ Time Series & Autocorrelation](https://online.stat.psu.edu/stat501/book/export/html/995 )\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n    <p style=\"padding: 15px;\n              color:black;\">ðŸ“Œ Based on the above, we will choose our look-back period to be 12 months. We will, therefore, generate a data frame that has 13 columns, 1 column for each of the 12 months and the column for our dependent variable, difference in sales.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"# Regressive Modeling\n\n> Let's create a CSV file where each row represents a month and the columns will have sales, dependent variables, and previous sales for each delay. The 12 delay properties are created according to the EDA. Data is used for regression modeling.\n\n\nCode was taken from [Baris Karamanâ€™s 'Data Driven Growth' series.](https://towardsdatascience.com/predicting-sales-611cb5a252de)","metadata":{}},{"cell_type":"code","source":"# Let's create a data frame for transformation from time series to supervised:\n\ndef built_supervised(data):\n    supervised_df = data.copy()\n\n    # Create column for each lag:\n    for i in range(1, 13):\n        col_name = 'lag_' + str(i)\n        supervised_df[col_name] = supervised_df['sales_diff'].shift(i)\n\n    # Drop null values:\n    supervised_df = supervised_df.dropna().reset_index(drop=True)\n\n    supervised_df.to_csv('./model_df.csv', index=False)\n    \n    return supervised_df\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.119148Z","iopub.execute_input":"2022-09-28T19:11:41.11967Z","iopub.status.idle":"2022-09-28T19:11:41.126284Z","shell.execute_reply.started":"2022-09-28T19:11:41.119636Z","shell.execute_reply":"2022-09-28T19:11:41.125263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_df = built_supervised(stationary_df)\nmodel_df ","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.127519Z","iopub.execute_input":"2022-09-28T19:11:41.128469Z","iopub.status.idle":"2022-09-28T19:11:41.225335Z","shell.execute_reply.started":"2022-09-28T19:11:41.128435Z","shell.execute_reply":"2022-09-28T19:11:41.22412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_df.info() # Supervised Dataframe","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.23131Z","iopub.execute_input":"2022-09-28T19:11:41.231702Z","iopub.status.idle":"2022-09-28T19:11:41.247532Z","shell.execute_reply.started":"2022-09-28T19:11:41.231666Z","shell.execute_reply":"2022-09-28T19:11:41.246164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n    <h5 style=\"padding: 15px;\n              color:black;\">ðŸ“Œ Now we have two separate data structures: <br> <br><br>\n              âœ” Our Arima structure which includes a DataTime index,\n        <br><br>\n             âœ” Our supervised structure which includes lags as features.\n    </h5>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## **Functions For Modeling** ","metadata":{}},{"cell_type":"markdown","source":"To create and assess all of our models, we will use a series of helper functions that perform the following functions:\n* *Train test split*\n* *Scale the data*\n* *Reverse scaling*\n* *Create a predictions data frame*\n* *Score the models*\n","metadata":{}},{"cell_type":"markdown","source":" # Train Test Split\n\n>       We detach our data so that the last 12 months are part of the test set and the rest of the data is used to train our model.","metadata":{}},{"cell_type":"code","source":"def train_test_split(data):\n    data = data.drop(['sales','date'], axis=1)\n    train , test = data[:-12].values, data[-12:].values\n    \n    return train, test\n\ntrain, test = train_test_split(model_df)\nprint(f\"Shape of  Train: {train.shape}\\nShape of  Test: {test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.249672Z","iopub.execute_input":"2022-09-28T19:11:41.250036Z","iopub.status.idle":"2022-09-28T19:11:41.261549Z","shell.execute_reply.started":"2022-09-28T19:11:41.250004Z","shell.execute_reply":"2022-09-28T19:11:41.260073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Scale The Data\n\n>      Using a min-max scaler, we will scale the data so that all of our variables fall within the range of -1 to 1.","metadata":{}},{"cell_type":"code","source":"def scale_data(train_set,test_set):\n    \"\"\"Scales data using MinMaxScaler and separates data into X_train, y_train,\n    X_test, and y_test.\"\"\"\n    \n    # Apply Min Max Scaler:\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler = scaler.fit(train_set)\n    \n    # Reshape training set:\n    train_set = train_set.reshape(train_set.shape[0],\n                                  train_set.shape[1])\n    train_set_scaled = scaler.transform(train_set)\n    \n    # Reshape test set:\n    test_set = test_set.reshape(test_set.shape[0], \n                                test_set.shape[1])\n    test_set_scaled = scaler.transform(test_set)\n    \n    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel() # returns the array, flattened!\n    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()\n    \n    return X_train, y_train, X_test, y_test, scaler\n\n\nX_train, y_train, X_test, y_test, scaler_object = scale_data(train, test)\nprint(f\"Shape of X Train: {X_train.shape}\\nShape of y Train: {y_train.shape}\\nShape of X Test: {X_test.shape}\\nShape of y Test: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.263005Z","iopub.execute_input":"2022-09-28T19:11:41.263859Z","iopub.status.idle":"2022-09-28T19:11:41.276472Z","shell.execute_reply.started":"2022-09-28T19:11:41.263803Z","shell.execute_reply":"2022-09-28T19:11:41.275161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Reverse Scaling\n>       After running our models, we will use this helper function to reverse the scaling of step 2.","metadata":{}},{"cell_type":"code","source":"def re_scaling(y_pred, x_test, scaler_obj, lstm=False):\n    \"\"\"For visualizing and comparing results, undoes the scaling effect on predictions.\"\"\"\n   # y_pred: model predictions\n   # x_test: features from the test set used for predictions\n   # scaler_obj: the scaler objects used for min-max scaling\n   # lstm: indicate if the model run is the lstm. If True, additional transformation occurs \n    \n    # Reshape y_pred:\n    y_pred = y_pred.reshape(y_pred.shape[0],\n                            1,\n                            1)\n\n    if not lstm:\n        x_test = x_test.reshape(x_test.shape[0],\n                                1, \n                                x_test.shape[1])\n\n    # Rebuild test set for inverse transform:\n    pred_test_set = []\n    for index in range(0, len(y_pred)):\n        pred_test_set.append(np.concatenate([y_pred[index], \n                                             x_test[index]],\n                                             axis=1) )\n\n    # Reshape pred_test_set:\n    pred_test_set = np.array(pred_test_set)\n    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0],\n                                          pred_test_set.shape[2])\n\n    # Inverse transform:\n    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)\n\n    return pred_test_set_inverted\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.278517Z","iopub.execute_input":"2022-09-28T19:11:41.278892Z","iopub.status.idle":"2022-09-28T19:11:41.28886Z","shell.execute_reply.started":"2022-09-28T19:11:41.278859Z","shell.execute_reply":"2022-09-28T19:11:41.287371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # Predictions Dataframe\n>     Generate a dataframe that includes the actual sales captured in our test set and the predicted results from our model so that we can quantify our success.","metadata":{}},{"cell_type":"code","source":"def prediction_df(unscale_predictions, origin_df):\n    \"\"\"Generates a dataframe that shows the predicted sales for each month\n    for plotting results.\"\"\"\n    \n    # unscale_predictions: the model predictions that do not have min-max or other scaling applied\n    # origin_df: the original monthly sales dataframe\n    \n    # Create dataframe that shows the predicted sales:\n    result_list = []\n    sales_dates = list(origin_df[-13:].date)\n    act_sales = list(origin_df[-13:].sales)\n\n    for index in range(0, len(unscale_predictions)):\n        result_dict = {}\n        result_dict['pred_value'] = int(unscale_predictions[index][0] + act_sales[index])\n        result_dict['date'] = sales_dates[index + 1]\n        result_list.append(result_dict)\n\n    df_result = pd.DataFrame(result_list)\n\n    return df_result","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.290531Z","iopub.execute_input":"2022-09-28T19:11:41.290989Z","iopub.status.idle":"2022-09-28T19:11:41.305596Z","shell.execute_reply.started":"2022-09-28T19:11:41.290946Z","shell.execute_reply":"2022-09-28T19:11:41.304677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Score The Models\n>      This helper function will save the root mean squared error (RMSE) and mean absolute error (MAE) of our predictions to compare the performance of our models.\n[Regression Metrics](https://scikit-learn.org/0.24/modules/model_evaluation.html#regression-metrics)","metadata":{}},{"cell_type":"markdown","source":"![](https://4.bp.blogspot.com/-wG7IbjTfE6k/XGUvqm7TCVI/AAAAAAAAAZU/vpH1kuKTIooKTcVlnm1EVRCXLVZM9cPNgCLcBGAs/s1600/formula-MAE-MSE-RMSE-RSquared.JPG)","metadata":{}},{"cell_type":"code","source":"model_scores = {}\n\ndef get_scores(unscale_df, origin_df, model_name):\n    \"\"\"Prints the root mean squared error, mean absolute error, and r2 scores\n    for each model. Saves all results in a model_scores dictionary for\n    comparison.\"\"\"\n    \n    rmse = np.sqrt(mean_squared_error(origin_df.sales[-12:], \n                                      unscale_df.pred_value[-12:]))\n    \n    mae = mean_absolute_error(origin_df.sales[-12:], \n                              unscale_df.pred_value[-12:])\n    \n    r2 = r2_score(origin_df.sales[-12:], \n                  unscale_df.pred_value[-12:])\n    \n    model_scores[model_name] = [rmse, mae, r2]\n\n    print(f\"RMSE: {rmse}\\nMAE: {mae}\\nR2 Score: {r2}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.306602Z","iopub.execute_input":"2022-09-28T19:11:41.306909Z","iopub.status.idle":"2022-09-28T19:11:41.322428Z","shell.execute_reply.started":"2022-09-28T19:11:41.306882Z","shell.execute_reply":"2022-09-28T19:11:41.321372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Graph of Results","metadata":{}},{"cell_type":"code","source":"def plot_results(results, origin_df, model_name):\n# results: a dataframe with unscaled predictions\n\n    fig, ax = plt.subplots(figsize=(15,5))\n    sns.lineplot(origin_df.date, origin_df.sales, data=origin_df, ax=ax, \n                 label='Original', color='blue')\n    sns.lineplot(results.date, results.pred_value, data=results, ax=ax, \n                 label='Predicted', color='red')\n    \n    \n    ax.set(xlabel = \"Date\",\n           ylabel = \"Sales\",\n           title = f\"{model_name} Sales Forecasting Prediction\")\n    \n    ax.legend(loc='best')\n    \n    filepath = Path('./model_output/{model_name}_forecasting.svg')  \n    filepath.parent.mkdir(parents=True, exist_ok=True) \n    plt.savefig(f'./model_output/{model_name}_forecasting.svg')","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.324106Z","iopub.execute_input":"2022-09-28T19:11:41.325278Z","iopub.status.idle":"2022-09-28T19:11:41.335806Z","shell.execute_reply.started":"2022-09-28T19:11:41.325235Z","shell.execute_reply":"2022-09-28T19:11:41.334039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def regressive_model(train_data, test_data, model, model_name):\n    \"\"\"Runs regressive models in SKlearn framework. First calls scale_data\n    to split into X and y and scale the data. Then fits and predicts. Finally,\n    predictions are unscaled, scores are printed, and results are plotted and\n    saved.\"\"\"\n    \n    # Split into X & y and scale data:\n    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data,\n                                                                 test_data)\n\n    # Run sklearn models:\n    mod = model\n    mod.fit(X_train, y_train)\n    predictions = mod.predict(X_test) # y_pred=predictions\n\n    # Undo scaling to compare predictions against original data:\n    origin_df = m_df\n    unscaled = re_scaling(predictions, X_test, scaler_object) # unscaled_predictions\n    unscaled_df = prediction_df(unscaled, origin_df)\n\n    # Print scores and plot results:\n    get_scores(unscaled_df, origin_df, model_name)\n    plot_results(unscaled_df, origin_df, model_name)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.337443Z","iopub.execute_input":"2022-09-28T19:11:41.337847Z","iopub.status.idle":"2022-09-28T19:11:41.352859Z","shell.execute_reply.started":"2022-09-28T19:11:41.337816Z","shell.execute_reply":"2022-09-28T19:11:41.351858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modeling**\n# Regressive Models\n* Linear Regression\n* Random Forest Regressor\n* XGBoost\n* LSTM","metadata":{}},{"cell_type":"markdown","source":"# [Linear Regression](http://)","metadata":{}},{"cell_type":"code","source":"regressive_model(train, test, LinearRegression(), 'LinearRegression')","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.353894Z","iopub.execute_input":"2022-09-28T19:11:41.35425Z","iopub.status.idle":"2022-09-28T19:11:41.805471Z","shell.execute_reply.started":"2022-09-28T19:11:41.354219Z","shell.execute_reply":"2022-09-28T19:11:41.804545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" # [ Random Forest Regressor ](https://scikit-learn.org/0.24/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest%20reg#sklearn.ensemble.RandomForestRegressor)","metadata":{}},{"cell_type":"code","source":"regressive_model(train, test, RandomForestRegressor(n_estimators=100, max_depth=20), \n          'RandomForest')","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:41.806919Z","iopub.execute_input":"2022-09-28T19:11:41.807543Z","iopub.status.idle":"2022-09-28T19:11:42.34584Z","shell.execute_reply.started":"2022-09-28T19:11:41.80751Z","shell.execute_reply":"2022-09-28T19:11:42.344801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [  XGBoost](https://xgboost.readthedocs.io/en/stable/parameter.html)","metadata":{}},{"cell_type":"code","source":"regressive_model(train, test, XGBRegressor(n_estimators=100,max_depth=3, \n                                           learning_rate=0.2,objective='reg:squarederror'), 'XGBoost')","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:42.347615Z","iopub.execute_input":"2022-09-28T19:11:42.348472Z","iopub.status.idle":"2022-09-28T19:11:43.113666Z","shell.execute_reply.started":"2022-09-28T19:11:42.348434Z","shell.execute_reply":"2022-09-28T19:11:43.11238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n\n> LSTM is a type of recurring neural network that is especially useful for predicting sequential data. [Getting started with the Keras Sequential model](https://faroit.com/keras-docs/1.2.0/getting-started/sequential-model-guide/#sequence-classification-with-lstm)","metadata":{}},{"cell_type":"code","source":"def lstm_model(train_data, test_data):\n    \"\"\"Runs a long-short-term-memory neural net with 2 dense layers. \n    Generates predictions that are then unscaled. \n    Scores are printed and the results are plotted and saved.\"\"\"\n    # train_data: dataset used to train the model\n    # test_data: dataset used to test the model\n   \n    \n    # Split into X & y and scale data:\n    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)\n    \n    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n   \n    \n    # Build LSTM:\n    model = Sequential()\n    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), \n                   stateful=True))\n    model.add(Dense(1))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, \n              shuffle=False)\n    predictions = model.predict(X_test,batch_size=1)\n    \n    # Undo scaling to compare predictions against original data:\n    origin_df = m_df\n    unscaled = re_scaling(predictions, X_test, scaler_object, lstm=True)\n    unscaled_df = prediction_df(unscaled, origin_df)\n    \n    get_scores(unscaled_df, origin_df, 'LSTM')\n    plot_results(unscaled_df, origin_df, 'LSTM')\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:43.115135Z","iopub.execute_input":"2022-09-28T19:11:43.116134Z","iopub.status.idle":"2022-09-28T19:11:43.127494Z","shell.execute_reply.started":"2022-09-28T19:11:43.116094Z","shell.execute_reply":"2022-09-28T19:11:43.126225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_model(train,test)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:43.128776Z","iopub.execute_input":"2022-09-28T19:11:43.129098Z","iopub.status.idle":"2022-09-28T19:11:49.665515Z","shell.execute_reply.started":"2022-09-28T19:11:43.12907Z","shell.execute_reply":"2022-09-28T19:11:49.664346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(model_scores, open( \"model_scores.p\", \"wb\" ))","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:49.666757Z","iopub.execute_input":"2022-09-28T19:11:49.667068Z","iopub.status.idle":"2022-09-28T19:11:49.673125Z","shell.execute_reply.started":"2022-09-28T19:11:49.667037Z","shell.execute_reply":"2022-09-28T19:11:49.671939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ARIMA Modeling\n\n[ **SARIMAX Modeling**  ](https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/)\n\n> We use the statsmodels SARIMAX package to train the model and generate dynamic predictions. The SARIMA model breaks down into a few parts.\n\n     AR: represented as p, is the autoregressive model\n     I : represented as d, is the differencing term\n     MA: represented as q, is the moving average model\n     S: enables us to add a seasonal component\n","metadata":{}},{"cell_type":"code","source":"datatime_df.index = pd.to_datetime(datatime_df.index)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:49.675043Z","iopub.execute_input":"2022-09-28T19:11:49.675389Z","iopub.status.idle":"2022-09-28T19:11:49.685938Z","shell.execute_reply.started":"2022-09-28T19:11:49.675358Z","shell.execute_reply":"2022-09-28T19:11:49.684531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n    <p style=\"padding: 15px;\n              color:black;\">ðŸ“Œ In the code below, we define our model and then make dynamic predictions for the last 12 months of the data. For standard, non-dynamic predictions, the following monthâ€™s prediction is made using the actual sales from the prior months. In contrast, for dynamic predictions, the following monthâ€™s prediction is made using the predicted sales from the prior months.\n    </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"def sarimax_model(data):\n    # Model:\n    sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(12, 0, 0),\n                                    seasonal_order=(0, 1, 0, 12),\n                                    trend='c').fit()\n    \n    # Generate predictions:\n    start, end, dynamic = 40, 100, 7\n    data['pred_value'] = sar.predict(start=start, end=end, dynamic=dynamic)\n    pred_df = data.pred_value[start+dynamic:end]\n    \n    data[[\"sales_diff\",\"pred_value\"]].plot(color=['blue', 'Red'])\n    plt.legend(loc='upper left')\n    \n    model_score = {}\n    \n    rmse = np.sqrt(mean_squared_error(data.sales_diff[-12:], data.pred_value[-12:]))\n    mae = mean_absolute_error(data.sales_diff[-12:], data.pred_value[-12:])\n    r2 = r2_score(data.sales_diff[-12:], data.pred_value[-12:])\n    model_scores['ARIMA'] = [rmse, mae, r2]\n    \n    print(f\"RMSE: {rmse}\\nMAE: {mae}\\nR2 Score: {r2}\")\n    \n    return sar, data, pred_df\n","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:39:01.681763Z","iopub.execute_input":"2022-09-28T19:39:01.682986Z","iopub.status.idle":"2022-09-28T19:39:01.693573Z","shell.execute_reply.started":"2022-09-28T19:39:01.682941Z","shell.execute_reply":"2022-09-28T19:39:01.691874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sar, datatime_df, predictions = sarimax_model(datatime_df)","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:38:39.62095Z","iopub.execute_input":"2022-09-28T19:38:39.621394Z","iopub.status.idle":"2022-09-28T19:38:39.675106Z","shell.execute_reply.started":"2022-09-28T19:38:39.62136Z","shell.execute_reply":"2022-09-28T19:38:39.67338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.plot_diagnostics.html)","metadata":{}},{"cell_type":"code","source":"sar.plot_diagnostics(figsize=(12, 8));","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:51.028095Z","iopub.execute_input":"2022-09-28T19:11:51.028531Z","iopub.status.idle":"2022-09-28T19:11:52.006526Z","shell.execute_reply.started":"2022-09-28T19:11:51.028496Z","shell.execute_reply":"2022-09-28T19:11:52.005187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pickle.dump(model_scores, open( \"ARIMAmodel_scores.p\", \"wb\" ))","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:52.008102Z","iopub.execute_input":"2022-09-28T19:11:52.008493Z","iopub.status.idle":"2022-09-28T19:11:52.014205Z","shell.execute_reply.started":"2022-09-28T19:11:52.008452Z","shell.execute_reply":"2022-09-28T19:11:52.012645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Comparing Models**","metadata":{}},{"cell_type":"code","source":"def create_results_df():\n    results_dict = pickle.load(open(\"model_scores.p\", \"rb\"))\n    \n    results_dict.update(pickle.load(open(\"ARIMAmodel_scores.p\", \"rb\")))\n    \n    results_df = pd.DataFrame.from_dict(results_dict, orient='index', \n                                        columns=['RMSE', 'MAE','R2'])\n    \n    results_df = results_df.sort_values(by='RMSE', ascending=False).reset_index()\n    \n    results_df.to_csv('./results.csv')\n    \n    fig, ax = plt.subplots(figsize=(12, 5))\n    sns.lineplot(np.arange(len(results_df)), 'RMSE', data=results_df, ax=ax, \n                 label='RMSE', color='darkblue')\n    sns.lineplot(np.arange(len(results_df)), 'MAE', data=results_df, ax=ax, \n                 label='MAE', color='Cyan')\n    \n    plt.xticks(np.arange(len(results_df)),rotation=45)\n    ax.set_xticklabels(results_df['index'])\n    ax.set(xlabel = \"Model\",\n           ylabel = \"Scores\",\n           title = \"Model Error Comparison\")\n    sns.despine()\n    \n    plt.savefig(f'./model_output/compare_models.png')\n    \n    return results_df\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:52.016003Z","iopub.execute_input":"2022-09-28T19:11:52.016452Z","iopub.status.idle":"2022-09-28T19:11:52.027589Z","shell.execute_reply.started":"2022-09-28T19:11:52.016392Z","shell.execute_reply":"2022-09-28T19:11:52.026367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = create_results_df()\nresults","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:52.029074Z","iopub.execute_input":"2022-09-28T19:11:52.03017Z","iopub.status.idle":"2022-09-28T19:11:52.397584Z","shell.execute_reply.started":"2022-09-28T19:11:52.030124Z","shell.execute_reply":"2022-09-28T19:11:52.396285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avarage_12months()","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:52.398881Z","iopub.execute_input":"2022-09-28T19:11:52.399598Z","iopub.status.idle":"2022-09-28T19:11:52.405747Z","shell.execute_reply.started":"2022-09-28T19:11:52.399563Z","shell.execute_reply":"2022-09-28T19:11:52.404747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"average = 894478.3333333334\nXGBoost = results.MAE.values[4]\npercentage_off = round(XGBoost/average*100,2)\n\nprint(f\"With XGBoost, prediction is within {percentage_off}% of the actual.\")","metadata":{"execution":{"iopub.status.busy":"2022-09-28T19:11:52.407057Z","iopub.execute_input":"2022-09-28T19:11:52.407405Z","iopub.status.idle":"2022-09-28T19:11:52.416874Z","shell.execute_reply.started":"2022-09-28T19:11:52.40737Z","shell.execute_reply":"2022-09-28T19:11:52.415835Z"},"trusted":true},"execution_count":null,"outputs":[]}]}