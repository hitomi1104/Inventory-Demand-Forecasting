{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Future Sales Using Machine Learning\n",
    "\n",
    "Forecasting future sales of a product offers many advantages. Predicting future sales of a product helps a company manage the cost of manufacturing and marketing the product. In this notebook, I will try to you through the task of future sales prediction with machine learning using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:28.334615Z",
     "iopub.status.busy": "2022-09-28T19:11:28.333232Z",
     "iopub.status.idle": "2022-09-28T19:11:36.257768Z",
     "shell.execute_reply": "2022-09-28T19:11:36.256584Z",
     "shell.execute_reply.started": "2022-09-28T19:11:28.334479Z"
    }
   },
   "outputs": [],
   "source": [
    "# EDA Libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.colors as col\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path  \n",
    "import random\n",
    "\n",
    "# Scikit-Learn models:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "# LSTM:\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "# ARIMA Model:\n",
    "\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "\n",
    "import pickle\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.260548Z",
     "iopub.status.busy": "2022-09-28T19:11:36.259813Z",
     "iopub.status.idle": "2022-09-28T19:11:36.265685Z",
     "shell.execute_reply": "2022-09-28T19:11:36.264462Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.260514Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Dataset & Data Exploration (EDA)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   The first step is to load the data and transform it into a structure that we will then use for each of our models. In its raw form, each row of data represents a single day of sales at one of ten stores. Our goal is to predict monthly sales, so we will first consolidate all stores and days into total monthly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.267634Z",
     "iopub.status.busy": "2022-09-28T19:11:36.26715Z",
     "iopub.status.idle": "2022-09-28T19:11:36.33136Z",
     "shell.execute_reply": "2022-09-28T19:11:36.330112Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.267578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>...</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>2016/11/08</td>\n",
       "      <td>2016/11/11</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>...</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-BO-10001798</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Bookcases</td>\n",
       "      <td>Bush Somerset Collection Bookcase</td>\n",
       "      <td>261.9600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CA-2016-152156</td>\n",
       "      <td>2016/11/08</td>\n",
       "      <td>2016/11/11</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>CG-12520</td>\n",
       "      <td>Claire Gute</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>...</td>\n",
       "      <td>42420</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-CH-10000454</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Hon Deluxe Fabric Upholstered Stacking Chairs,...</td>\n",
       "      <td>731.9400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>219.5820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>CA-2016-138688</td>\n",
       "      <td>2016/06/12</td>\n",
       "      <td>2016/06/16</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>DV-13045</td>\n",
       "      <td>Darrin Van Huff</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>United States</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>90036</td>\n",
       "      <td>West</td>\n",
       "      <td>OFF-LA-10000240</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Self-Adhesive Address Labels for Typewriters b...</td>\n",
       "      <td>14.6200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.8714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>2015/10/11</td>\n",
       "      <td>2015/10/18</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>...</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>FUR-TA-10000577</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Tables</td>\n",
       "      <td>Bretford CR4500 Series Slim Rectangular Table</td>\n",
       "      <td>957.5775</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-383.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2015-108966</td>\n",
       "      <td>2015/10/11</td>\n",
       "      <td>2015/10/18</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>SO-20335</td>\n",
       "      <td>Sean O'Donnell</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Fort Lauderdale</td>\n",
       "      <td>...</td>\n",
       "      <td>33311</td>\n",
       "      <td>South</td>\n",
       "      <td>OFF-ST-10000760</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>Eldon Fold 'N Roll Cart System</td>\n",
       "      <td>22.3680</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.5164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       1  CA-2016-152156  2016/11/08  2016/11/11    Second Class    CG-12520   \n",
       "1       2  CA-2016-152156  2016/11/08  2016/11/11    Second Class    CG-12520   \n",
       "2       3  CA-2016-138688  2016/06/12  2016/06/16    Second Class    DV-13045   \n",
       "3       4  US-2015-108966  2015/10/11  2015/10/18  Standard Class    SO-20335   \n",
       "4       5  US-2015-108966  2015/10/11  2015/10/18  Standard Class    SO-20335   \n",
       "\n",
       "     Customer Name    Segment        Country             City  ...  \\\n",
       "0      Claire Gute   Consumer  United States        Henderson  ...   \n",
       "1      Claire Gute   Consumer  United States        Henderson  ...   \n",
       "2  Darrin Van Huff  Corporate  United States      Los Angeles  ...   \n",
       "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
       "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale  ...   \n",
       "\n",
       "  Postal Code  Region       Product ID         Category Sub-Category  \\\n",
       "0       42420   South  FUR-BO-10001798        Furniture    Bookcases   \n",
       "1       42420   South  FUR-CH-10000454        Furniture       Chairs   \n",
       "2       90036    West  OFF-LA-10000240  Office Supplies       Labels   \n",
       "3       33311   South  FUR-TA-10000577        Furniture       Tables   \n",
       "4       33311   South  OFF-ST-10000760  Office Supplies      Storage   \n",
       "\n",
       "                                        Product Name     Sales  Quantity  \\\n",
       "0                  Bush Somerset Collection Bookcase  261.9600         2   \n",
       "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400         3   \n",
       "2  Self-Adhesive Address Labels for Typewriters b...   14.6200         2   \n",
       "3      Bretford CR4500 Series Slim Rectangular Table  957.5775         5   \n",
       "4                     Eldon Fold 'N Roll Cart System   22.3680         2   \n",
       "\n",
       "   Discount    Profit  \n",
       "0      0.00   41.9136  \n",
       "1      0.00  219.5820  \n",
       "2      0.00    6.8714  \n",
       "3      0.45 -383.0310  \n",
       "4      0.20    2.5164  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../input/demand-forecasting-kernels-only/sample_submission.csv')\n",
    "df = dataset.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.334911Z",
     "iopub.status.busy": "2022-09-28T19:11:36.334422Z",
     "iopub.status.idle": "2022-09-28T19:11:36.340923Z",
     "shell.execute_reply": "2022-09-28T19:11:36.339757Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.334865Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    \"\"\"Returns a pandas dataframe from a csv file.\"\"\"\n",
    "    return pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:36.343382Z",
     "iopub.status.busy": "2022-09-28T19:11:36.34287Z",
     "iopub.status.idle": "2022-09-28T19:11:37.050788Z",
     "shell.execute_reply": "2022-09-28T19:11:37.04965Z",
     "shell.execute_reply.started": "2022-09-28T19:11:36.343331Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/demand-forecasting-kernels-only/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sales_data \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/demand-forecasting-kernels-only/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_s \u001b[38;5;241m=\u001b[39m sales_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      3\u001b[0m df_s\u001b[38;5;241m.\u001b[39minfo()\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_name):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a pandas dataframe from a csv file.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/demand-forecasting-kernels-only/train.csv'"
     ]
    }
   ],
   "source": [
    "sales_data = load_data('../input/demand-forecasting-kernels-only/train.csv')\n",
    "df_s = sales_data.copy()\n",
    "df_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.052694Z",
     "iopub.status.busy": "2022-09-28T19:11:37.052373Z",
     "iopub.status.idle": "2022-09-28T19:11:37.06468Z",
     "shell.execute_reply": "2022-09-28T19:11:37.063493Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.052666Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_s\u001b[38;5;241m.\u001b[39mtail()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_s' is not defined"
     ]
    }
   ],
   "source": [
    "df_s.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.066938Z",
     "iopub.status.busy": "2022-09-28T19:11:37.066191Z",
     "iopub.status.idle": "2022-09-28T19:11:37.117364Z",
     "shell.execute_reply": "2022-09-28T19:11:37.116195Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.066895Z"
    }
   },
   "outputs": [],
   "source": [
    "# To view basic statistical details about dataset:\n",
    "\n",
    "df_s['sales'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <h4 style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Sales seem to be unbalanced!\n",
    "    </h4>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.120067Z",
     "iopub.status.busy": "2022-09-28T19:11:37.1196Z",
     "iopub.status.idle": "2022-09-28T19:11:37.650172Z",
     "shell.execute_reply": "2022-09-28T19:11:37.649108Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.120012Z"
    }
   },
   "outputs": [],
   "source": [
    "df_s['sales'].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Returns a dataframe where each row represents total sales for a given month. Columns include 'date' by month and 'sales'.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.651889Z",
     "iopub.status.busy": "2022-09-28T19:11:37.651552Z",
     "iopub.status.idle": "2022-09-28T19:11:37.659058Z",
     "shell.execute_reply": "2022-09-28T19:11:37.657963Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.651858Z"
    }
   },
   "outputs": [],
   "source": [
    "def monthlyORyears_sales(data,time=['monthly','years']):\n",
    "    data = data.copy()\n",
    "    if time == \"monthly\":\n",
    "        # Drop the day indicator from the date column:\n",
    "        data.date = data.date.apply(lambda x: str(x)[:-3])\n",
    "    else:\n",
    "        data.date = data.date.apply(lambda x: str(x)[:4])\n",
    "        \n",
    "   # Sum sales per month: \n",
    "    data = data.groupby('date')['sales'].sum().reset_index()\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "        \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:37.662972Z",
     "iopub.status.busy": "2022-09-28T19:11:37.662607Z",
     "iopub.status.idle": "2022-09-28T19:11:38.171218Z",
     "shell.execute_reply": "2022-09-28T19:11:38.170417Z",
     "shell.execute_reply.started": "2022-09-28T19:11:37.662928Z"
    }
   },
   "outputs": [],
   "source": [
    "m_df = monthlyORyears_sales(df_s,\"monthly\")\n",
    "\n",
    "m_df.to_csv('./monthly_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:38.173179Z",
     "iopub.status.busy": "2022-09-28T19:11:38.172628Z",
     "iopub.status.idle": "2022-09-28T19:11:38.183395Z",
     "shell.execute_reply": "2022-09-28T19:11:38.182255Z",
     "shell.execute_reply.started": "2022-09-28T19:11:38.173145Z"
    }
   },
   "outputs": [],
   "source": [
    "m_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In our new data frame, each row now represents the total sales for a given month across stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:38.184808Z",
     "iopub.status.busy": "2022-09-28T19:11:38.184519Z",
     "iopub.status.idle": "2022-09-28T19:11:38.676214Z",
     "shell.execute_reply": "2022-09-28T19:11:38.675384Z",
     "shell.execute_reply.started": "2022-09-28T19:11:38.184782Z"
    }
   },
   "outputs": [],
   "source": [
    "y_df = monthlyORyears_sales(df_s,\"years\")\n",
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:38.67754Z",
     "iopub.status.busy": "2022-09-28T19:11:38.67722Z",
     "iopub.status.idle": "2022-09-28T19:11:39.19411Z",
     "shell.execute_reply": "2022-09-28T19:11:39.192798Z",
     "shell.execute_reply.started": "2022-09-28T19:11:38.677512Z"
    }
   },
   "outputs": [],
   "source": [
    "layout = (1, 2)\n",
    "\n",
    "raw = plt.subplot2grid(layout, (0 ,0))\n",
    "law = plt.subplot2grid(layout, (0 ,1))\n",
    "\n",
    "years = y_df['sales'].plot(kind = \"bar\",color = 'mediumblue', label=\"Sales\",ax=raw, figsize=(12,5))\n",
    "months = m_df['sales'].plot(marker = 'o',color = 'darkorange', label=\"Sales\", ax=law)\n",
    "\n",
    "years.set(xlabel = \"Years\",title = \"Distribution of Sales Per Year\")\n",
    "months.set(xlabel = \"Months\", title = \"Distribution of Sales Per Mounth\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "years.legend()\n",
    "months.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <p style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ If we plot the total monthly sales over time, we see that the average monthly sales increase over time, so our data is not stationary.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ðŸ¦Ž TREND AND SEASONALITY](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n",
    "> There are many other models to forecast time series, such as weighted\n",
    "moving average models or autoregressive integrated moving average\n",
    "(ARIMA) models. Some of them require you to first remove the trend\n",
    "and seasonality. For example, if you are studying the number of active\n",
    "users on your website, and it is growing by 10% every month, you\n",
    "would have to remove this trend from the time series. Once the model\n",
    "is trained and starts making predictions, you would have to add the\n",
    "trend back to get the final predictions. Similarly, if you are trying to\n",
    "predict the amount of sunscreen lotion sold every month, you will\n",
    "probably observe strong seasonality: since it sells well every summer,\n",
    "a similar pattern will be repeated every year. You would have to\n",
    "remove this seasonality from the time series, for example by\n",
    "computing the difference between the value at each time step and the\n",
    "value one year earlier (this technique is called differencing). Again,\n",
    "after the model is trained and makes predictions, you would have to\n",
    "add the seasonal pattern back to get the final predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Exploration (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To make it stationary, we will calculate the difference between the sales of each month and add it to our data frame as a new column.\n",
    "> Further details of the stationary and differences are available [here.](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/) But first let's take a closer look at the data set you work with by learning more about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.195933Z",
     "iopub.status.busy": "2022-09-28T19:11:39.195555Z",
     "iopub.status.idle": "2022-09-28T19:11:39.202363Z",
     "shell.execute_reply": "2022-09-28T19:11:39.201059Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.1959Z"
    }
   },
   "outputs": [],
   "source": [
    "def sales_time(data):\n",
    "    \"\"\"Time interval of dataset:\"\"\"\n",
    "\n",
    "    data.date = pd.to_datetime(data.date)\n",
    "    n_of_days = data.date.max() - data.date.min()\n",
    "    n_of_years = int(n_of_days.days / 365)\n",
    "    \n",
    "    print(f\"Days: {n_of_days.days}\\nYears: {n_of_years}\\nMonth: {12 * n_of_years}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.20501Z",
     "iopub.status.busy": "2022-09-28T19:11:39.204548Z",
     "iopub.status.idle": "2022-09-28T19:11:39.354247Z",
     "shell.execute_reply": "2022-09-28T19:11:39.353037Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.204968Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_time(df_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sales Data Per Store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.356381Z",
     "iopub.status.busy": "2022-09-28T19:11:39.355978Z",
     "iopub.status.idle": "2022-09-28T19:11:39.364014Z",
     "shell.execute_reply": "2022-09-28T19:11:39.363068Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.356332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's sell it per store:\n",
    "\n",
    "def sales_per_store(data):\n",
    "    sales_by_store = data.groupby('store')['sales'].sum().reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    sns.barplot(sales_by_store.store, sales_by_store.sales, color='darkred')\n",
    "    \n",
    "    ax.set(xlabel = \"Store Id\", ylabel = \"Sum of Sales\", title = \"Total Sales Per Store\")\n",
    "    \n",
    "    return sales_by_store\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.36568Z",
     "iopub.status.busy": "2022-09-28T19:11:39.36518Z",
     "iopub.status.idle": "2022-09-28T19:11:39.664247Z",
     "shell.execute_reply": "2022-09-28T19:11:39.663088Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.365647Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_per_store(df_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Monthly Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.666317Z",
     "iopub.status.busy": "2022-09-28T19:11:39.665827Z",
     "iopub.status.idle": "2022-09-28T19:11:39.674328Z",
     "shell.execute_reply": "2022-09-28T19:11:39.67322Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.666247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Overall for 5 years:\n",
    "\n",
    "average_m_sales = m_df.sales.mean()\n",
    "print(f\"Overall Avarage Monthly Sales: ${average_m_sales}\")\n",
    "\n",
    "def avarage_12months():\n",
    "# Last 1 years (this will be the forecasted sales):\n",
    "    average_m_sales_1y = m_df.sales[-12:].mean()\n",
    "    print(f\"Last 12 months average monthly sales: ${average_m_sales_1y}\")\n",
    "avarage_12months()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Determining Time Series Stationary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Then statistical forecasting techniques can be implemented in this series. The final step would be to convert the forecasted values into the original scale by applying trend and seasonality constraints back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.676324Z",
     "iopub.status.busy": "2022-09-28T19:11:39.67573Z",
     "iopub.status.idle": "2022-09-28T19:11:39.689424Z",
     "shell.execute_reply": "2022-09-28T19:11:39.688367Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.676278Z"
    }
   },
   "outputs": [],
   "source": [
    "def time_plot(data, x_col, y_col, title):\n",
    "    fig, ax = plt.subplots(figsize = (15,8))\n",
    "    sns.lineplot(x_col, y_col, data = data, ax = ax, color = 'darkblue', label='Total Sales')\n",
    "    \n",
    "    s_mean = data.groupby(data.date.dt.year)[y_col].mean().reset_index()\n",
    "    s_mean.date = pd.to_datetime(s_mean.date, format='%Y')\n",
    "    sns.lineplot((s_mean.date + datetime.timedelta(6*365/12)), y_col, data=s_mean, ax=ax, color='red', label='Mean Sales')   \n",
    "    \n",
    "    ax.set(xlabel = \"Years\",\n",
    "           ylabel = \"Sales\",\n",
    "           title = title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:39.691874Z",
     "iopub.status.busy": "2022-09-28T19:11:39.691161Z",
     "iopub.status.idle": "2022-09-28T19:11:40.068119Z",
     "shell.execute_reply": "2022-09-28T19:11:40.066881Z",
     "shell.execute_reply.started": "2022-09-28T19:11:39.691831Z"
    }
   },
   "outputs": [],
   "source": [
    "time_plot(m_df, 'date', 'sales', 'Monthly Sales Before Diff Transformation' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Differencing**\n",
    "\n",
    "In this method, we compute the difference of consecutive terms in the series. Differencing is typically performed to get rid of the varying mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.069828Z",
     "iopub.status.busy": "2022-09-28T19:11:40.069482Z",
     "iopub.status.idle": "2022-09-28T19:11:40.076522Z",
     "shell.execute_reply": "2022-09-28T19:11:40.075224Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.069799Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_diff(data):\n",
    "    \"\"\"Calculate the difference in sales month over month:\"\"\"\n",
    "    \n",
    "    data['sales_diff'] = data.sales.diff()\n",
    "    data = data.dropna()\n",
    "    \n",
    "    data.to_csv('./stationary_df.csv')\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.079688Z",
     "iopub.status.busy": "2022-09-28T19:11:40.078706Z",
     "iopub.status.idle": "2022-09-28T19:11:40.447236Z",
     "shell.execute_reply": "2022-09-28T19:11:40.446366Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.079644Z"
    }
   },
   "outputs": [],
   "source": [
    "stationary_df = get_diff(m_df)\n",
    "time_plot(stationary_df, 'date', 'sales_diff', \n",
    "          'Monthly Sales After Diff Transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <h5 style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Now that our data represent monthly sales and we have transformed it to be stationary, we will set up the \n",
    "                            data for our different model types. <br> <br>\n",
    "              âœ” To do this, we will \n",
    "                            define two different structures: \n",
    "        <br> <br>\n",
    "                    1. one will be used for ARIMA modeling,\n",
    "        <br><br>\n",
    "                    2. the other will be used for the rest of the models.\n",
    "    </h5>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preparing Dataset Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling\n",
    "\n",
    ">  For our Arima model, we will need only a datetime index and the dependent variable (diff in sales) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.448968Z",
     "iopub.status.busy": "2022-09-28T19:11:40.448314Z",
     "iopub.status.idle": "2022-09-28T19:11:40.454539Z",
     "shell.execute_reply": "2022-09-28T19:11:40.453661Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.448935Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_arima_data(data):\n",
    "    \"\"\"Generates a csv-file with a datetime index and a dependent sales column for ARIMA modeling.\"\"\"\n",
    "    \n",
    "    da_data = data.set_index('date').drop('sales', axis=1)\n",
    "    da_data.dropna(axis=0)\n",
    "    \n",
    "    da_data.to_csv('./arima_df.csv')\n",
    "    \n",
    "    return da_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.457149Z",
     "iopub.status.busy": "2022-09-28T19:11:40.455979Z",
     "iopub.status.idle": "2022-09-28T19:11:40.483662Z",
     "shell.execute_reply": "2022-09-28T19:11:40.482544Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.457065Z"
    }
   },
   "outputs": [],
   "source": [
    "datatime_df = build_arima_data(stationary_df)\n",
    "datatime_df # ARIMA Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Observing Lags**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our other models, we will create a new data frame where each feature represents a previous monthâ€™s sales. To determine how many months to include in our feature set, we will observe the autocorrelation and partial autocorrelation plots and use the [rules for selecting lags in ARIMA modeling](https://people.duke.edu/~rnau/arimrule.htm). This way, we can keep consistent a look-back period for our ARIMA and regressive models.\n",
    "\n",
    ">  **[Statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:40.485415Z",
     "iopub.status.busy": "2022-09-28T19:11:40.48506Z",
     "iopub.status.idle": "2022-09-28T19:11:41.117606Z",
     "shell.execute_reply": "2022-09-28T19:11:41.116363Z",
     "shell.execute_reply.started": "2022-09-28T19:11:40.485385Z"
    }
   },
   "outputs": [],
   "source": [
    "def plots_lag(data, lags=None):\n",
    "    \"\"\"Convert dataframe to datetime index\"\"\"\n",
    "    dt_data = data.set_index('date').drop('sales', axis=1)\n",
    "    dt_data.dropna(axis=0)\n",
    "    \n",
    "    \n",
    "    law  = plt.subplot(122)\n",
    "    acf  = plt.subplot(221)\n",
    "    pacf = plt.subplot(223)\n",
    "    \n",
    "    dt_data.plot(ax=law, figsize=(10, 5), color='orange')\n",
    "    # Plot the autocorrelation function:\n",
    "    smt.graphics.plot_acf(dt_data, lags=lags, ax=acf, color='mediumblue')\n",
    "    smt.graphics.plot_pacf(dt_data, lags=lags, ax=pacf, color='mediumblue')\n",
    "    \n",
    "    # Will also adjust spacing between subplots to minimize the overlaps:\n",
    "    plt.tight_layout()\n",
    "\n",
    "plots_lag(stationary_df, lags=24);\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> [ðŸŽ¦ Autocorrelation Function](https://www.youtube.com/watch?v=ZjaBn93YPWo&t=12s)\n",
    "\n",
    "\n",
    ">[ Time Series & Autocorrelation](https://online.stat.psu.edu/stat501/book/export/html/995 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <p style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Based on the above, we will choose our look-back period to be 12 months. We will, therefore, generate a data frame that has 13 columns, 1 column for each of the 12 months and the column for our dependent variable, difference in sales.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressive Modeling\n",
    "\n",
    "> Let's create a CSV file where each row represents a month and the columns will have sales, dependent variables, and previous sales for each delay. The 12 delay properties are created according to the EDA. Data is used for regression modeling.\n",
    "\n",
    "\n",
    "Code was taken from [Baris Karamanâ€™s 'Data Driven Growth' series.](https://towardsdatascience.com/predicting-sales-611cb5a252de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.11967Z",
     "iopub.status.busy": "2022-09-28T19:11:41.119148Z",
     "iopub.status.idle": "2022-09-28T19:11:41.126284Z",
     "shell.execute_reply": "2022-09-28T19:11:41.125263Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.119636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a data frame for transformation from time series to supervised:\n",
    "\n",
    "def built_supervised(data):\n",
    "    supervised_df = data.copy()\n",
    "\n",
    "    # Create column for each lag:\n",
    "    for i in range(1, 13):\n",
    "        col_name = 'lag_' + str(i)\n",
    "        supervised_df[col_name] = supervised_df['sales_diff'].shift(i)\n",
    "\n",
    "    # Drop null values:\n",
    "    supervised_df = supervised_df.dropna().reset_index(drop=True)\n",
    "\n",
    "    supervised_df.to_csv('./model_df.csv', index=False)\n",
    "    \n",
    "    return supervised_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.128469Z",
     "iopub.status.busy": "2022-09-28T19:11:41.127519Z",
     "iopub.status.idle": "2022-09-28T19:11:41.225335Z",
     "shell.execute_reply": "2022-09-28T19:11:41.22412Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.128435Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df = built_supervised(stationary_df)\n",
    "model_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.231702Z",
     "iopub.status.busy": "2022-09-28T19:11:41.23131Z",
     "iopub.status.idle": "2022-09-28T19:11:41.247532Z",
     "shell.execute_reply": "2022-09-28T19:11:41.246164Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.231666Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df.info() # Supervised Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <h5 style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ Now we have two separate data structures: <br> <br><br>\n",
    "              âœ” Our Arima structure which includes a DataTime index,\n",
    "        <br><br>\n",
    "             âœ” Our supervised structure which includes lags as features.\n",
    "    </h5>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions For Modeling** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create and assess all of our models, we will use a series of helper functions that perform the following functions:\n",
    "* *Train test split*\n",
    "* *Scale the data*\n",
    "* *Reverse scaling*\n",
    "* *Create a predictions data frame*\n",
    "* *Score the models*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Train Test Split\n",
    "\n",
    ">       We detach our data so that the last 12 months are part of the test set and the rest of the data is used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.250036Z",
     "iopub.status.busy": "2022-09-28T19:11:41.249672Z",
     "iopub.status.idle": "2022-09-28T19:11:41.261549Z",
     "shell.execute_reply": "2022-09-28T19:11:41.260073Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.250004Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(data):\n",
    "    data = data.drop(['sales','date'], axis=1)\n",
    "    train , test = data[:-12].values, data[-12:].values\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = train_test_split(model_df)\n",
    "print(f\"Shape of  Train: {train.shape}\\nShape of  Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Scale The Data\n",
    "\n",
    ">      Using a min-max scaler, we will scale the data so that all of our variables fall within the range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.263859Z",
     "iopub.status.busy": "2022-09-28T19:11:41.263005Z",
     "iopub.status.idle": "2022-09-28T19:11:41.276472Z",
     "shell.execute_reply": "2022-09-28T19:11:41.275161Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.263803Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_data(train_set,test_set):\n",
    "    \"\"\"Scales data using MinMaxScaler and separates data into X_train, y_train,\n",
    "    X_test, and y_test.\"\"\"\n",
    "    \n",
    "    # Apply Min Max Scaler:\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train_set)\n",
    "    \n",
    "    # Reshape training set:\n",
    "    train_set = train_set.reshape(train_set.shape[0],\n",
    "                                  train_set.shape[1])\n",
    "    train_set_scaled = scaler.transform(train_set)\n",
    "    \n",
    "    # Reshape test set:\n",
    "    test_set = test_set.reshape(test_set.shape[0], \n",
    "                                test_set.shape[1])\n",
    "    test_set_scaled = scaler.transform(test_set)\n",
    "    \n",
    "    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel() # returns the array, flattened!\n",
    "    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler_object = scale_data(train, test)\n",
    "print(f\"Shape of X Train: {X_train.shape}\\nShape of y Train: {y_train.shape}\\nShape of X Test: {X_test.shape}\\nShape of y Test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Reverse Scaling\n",
    ">       After running our models, we will use this helper function to reverse the scaling of step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.278892Z",
     "iopub.status.busy": "2022-09-28T19:11:41.278517Z",
     "iopub.status.idle": "2022-09-28T19:11:41.28886Z",
     "shell.execute_reply": "2022-09-28T19:11:41.287371Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.278859Z"
    }
   },
   "outputs": [],
   "source": [
    "def re_scaling(y_pred, x_test, scaler_obj, lstm=False):\n",
    "    \"\"\"For visualizing and comparing results, undoes the scaling effect on predictions.\"\"\"\n",
    "   # y_pred: model predictions\n",
    "   # x_test: features from the test set used for predictions\n",
    "   # scaler_obj: the scaler objects used for min-max scaling\n",
    "   # lstm: indicate if the model run is the lstm. If True, additional transformation occurs \n",
    "    \n",
    "    # Reshape y_pred:\n",
    "    y_pred = y_pred.reshape(y_pred.shape[0],\n",
    "                            1,\n",
    "                            1)\n",
    "\n",
    "    if not lstm:\n",
    "        x_test = x_test.reshape(x_test.shape[0],\n",
    "                                1, \n",
    "                                x_test.shape[1])\n",
    "\n",
    "    # Rebuild test set for inverse transform:\n",
    "    pred_test_set = []\n",
    "    for index in range(0, len(y_pred)):\n",
    "        pred_test_set.append(np.concatenate([y_pred[index], \n",
    "                                             x_test[index]],\n",
    "                                             axis=1) )\n",
    "\n",
    "    # Reshape pred_test_set:\n",
    "    pred_test_set = np.array(pred_test_set)\n",
    "    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0],\n",
    "                                          pred_test_set.shape[2])\n",
    "\n",
    "    # Inverse transform:\n",
    "    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)\n",
    "\n",
    "    return pred_test_set_inverted\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Predictions Dataframe\n",
    ">     Generate a dataframe that includes the actual sales captured in our test set and the predicted results from our model so that we can quantify our success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.290989Z",
     "iopub.status.busy": "2022-09-28T19:11:41.290531Z",
     "iopub.status.idle": "2022-09-28T19:11:41.305596Z",
     "shell.execute_reply": "2022-09-28T19:11:41.304677Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.290946Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction_df(unscale_predictions, origin_df):\n",
    "    \"\"\"Generates a dataframe that shows the predicted sales for each month\n",
    "    for plotting results.\"\"\"\n",
    "    \n",
    "    # unscale_predictions: the model predictions that do not have min-max or other scaling applied\n",
    "    # origin_df: the original monthly sales dataframe\n",
    "    \n",
    "    # Create dataframe that shows the predicted sales:\n",
    "    result_list = []\n",
    "    sales_dates = list(origin_df[-13:].date)\n",
    "    act_sales = list(origin_df[-13:].sales)\n",
    "\n",
    "    for index in range(0, len(unscale_predictions)):\n",
    "        result_dict = {}\n",
    "        result_dict['pred_value'] = int(unscale_predictions[index][0] + act_sales[index])\n",
    "        result_dict['date'] = sales_dates[index + 1]\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "    df_result = pd.DataFrame(result_list)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score The Models\n",
    ">      This helper function will save the root mean squared error (RMSE) and mean absolute error (MAE) of our predictions to compare the performance of our models.\n",
    "[Regression Metrics](https://scikit-learn.org/0.24/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://4.bp.blogspot.com/-wG7IbjTfE6k/XGUvqm7TCVI/AAAAAAAAAZU/vpH1kuKTIooKTcVlnm1EVRCXLVZM9cPNgCLcBGAs/s1600/formula-MAE-MSE-RMSE-RSquared.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.306909Z",
     "iopub.status.busy": "2022-09-28T19:11:41.306602Z",
     "iopub.status.idle": "2022-09-28T19:11:41.322428Z",
     "shell.execute_reply": "2022-09-28T19:11:41.321372Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.306882Z"
    }
   },
   "outputs": [],
   "source": [
    "model_scores = {}\n",
    "\n",
    "def get_scores(unscale_df, origin_df, model_name):\n",
    "    \"\"\"Prints the root mean squared error, mean absolute error, and r2 scores\n",
    "    for each model. Saves all results in a model_scores dictionary for\n",
    "    comparison.\"\"\"\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(origin_df.sales[-12:], \n",
    "                                      unscale_df.pred_value[-12:]))\n",
    "    \n",
    "    mae = mean_absolute_error(origin_df.sales[-12:], \n",
    "                              unscale_df.pred_value[-12:])\n",
    "    \n",
    "    r2 = r2_score(origin_df.sales[-12:], \n",
    "                  unscale_df.pred_value[-12:])\n",
    "    \n",
    "    model_scores[model_name] = [rmse, mae, r2]\n",
    "\n",
    "    print(f\"RMSE: {rmse}\\nMAE: {mae}\\nR2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.325278Z",
     "iopub.status.busy": "2022-09-28T19:11:41.324106Z",
     "iopub.status.idle": "2022-09-28T19:11:41.335806Z",
     "shell.execute_reply": "2022-09-28T19:11:41.334039Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.325235Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_results(results, origin_df, model_name):\n",
    "# results: a dataframe with unscaled predictions\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    sns.lineplot(origin_df.date, origin_df.sales, data=origin_df, ax=ax, \n",
    "                 label='Original', color='blue')\n",
    "    sns.lineplot(results.date, results.pred_value, data=results, ax=ax, \n",
    "                 label='Predicted', color='red')\n",
    "    \n",
    "    \n",
    "    ax.set(xlabel = \"Date\",\n",
    "           ylabel = \"Sales\",\n",
    "           title = f\"{model_name} Sales Forecasting Prediction\")\n",
    "    \n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    filepath = Path('./model_output/{model_name}_forecasting.svg')  \n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True) \n",
    "    plt.savefig(f'./model_output/{model_name}_forecasting.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.337847Z",
     "iopub.status.busy": "2022-09-28T19:11:41.337443Z",
     "iopub.status.idle": "2022-09-28T19:11:41.352859Z",
     "shell.execute_reply": "2022-09-28T19:11:41.351858Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.337816Z"
    }
   },
   "outputs": [],
   "source": [
    "def regressive_model(train_data, test_data, model, model_name):\n",
    "    \"\"\"Runs regressive models in SKlearn framework. First calls scale_data\n",
    "    to split into X and y and scale the data. Then fits and predicts. Finally,\n",
    "    predictions are unscaled, scores are printed, and results are plotted and\n",
    "    saved.\"\"\"\n",
    "    \n",
    "    # Split into X & y and scale data:\n",
    "    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data,\n",
    "                                                                 test_data)\n",
    "\n",
    "    # Run sklearn models:\n",
    "    mod = model\n",
    "    mod.fit(X_train, y_train)\n",
    "    predictions = mod.predict(X_test) # y_pred=predictions\n",
    "\n",
    "    # Undo scaling to compare predictions against original data:\n",
    "    origin_df = m_df\n",
    "    unscaled = re_scaling(predictions, X_test, scaler_object) # unscaled_predictions\n",
    "    unscaled_df = prediction_df(unscaled, origin_df)\n",
    "\n",
    "    # Print scores and plot results:\n",
    "    get_scores(unscaled_df, origin_df, model_name)\n",
    "    plot_results(unscaled_df, origin_df, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**\n",
    "# Regressive Models\n",
    "* Linear Regression\n",
    "* Random Forest Regressor\n",
    "* XGBoost\n",
    "* LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Linear Regression](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.35425Z",
     "iopub.status.busy": "2022-09-28T19:11:41.353894Z",
     "iopub.status.idle": "2022-09-28T19:11:41.805471Z",
     "shell.execute_reply": "2022-09-28T19:11:41.804545Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.354219Z"
    }
   },
   "outputs": [],
   "source": [
    "regressive_model(train, test, LinearRegression(), 'LinearRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # [ Random Forest Regressor ](https://scikit-learn.org/0.24/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=random%20forest%20reg#sklearn.ensemble.RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:41.807543Z",
     "iopub.status.busy": "2022-09-28T19:11:41.806919Z",
     "iopub.status.idle": "2022-09-28T19:11:42.34584Z",
     "shell.execute_reply": "2022-09-28T19:11:42.344801Z",
     "shell.execute_reply.started": "2022-09-28T19:11:41.80751Z"
    }
   },
   "outputs": [],
   "source": [
    "regressive_model(train, test, RandomForestRegressor(n_estimators=100, max_depth=20), \n",
    "          'RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [  XGBoost](https://xgboost.readthedocs.io/en/stable/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:42.348472Z",
     "iopub.status.busy": "2022-09-28T19:11:42.347615Z",
     "iopub.status.idle": "2022-09-28T19:11:43.113666Z",
     "shell.execute_reply": "2022-09-28T19:11:43.11238Z",
     "shell.execute_reply.started": "2022-09-28T19:11:42.348434Z"
    }
   },
   "outputs": [],
   "source": [
    "regressive_model(train, test, XGBRegressor(n_estimators=100,max_depth=3, \n",
    "                                           learning_rate=0.2,objective='reg:squarederror'), 'XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n",
    "\n",
    "> LSTM is a type of recurring neural network that is especially useful for predicting sequential data. [Getting started with the Keras Sequential model](https://faroit.com/keras-docs/1.2.0/getting-started/sequential-model-guide/#sequence-classification-with-lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:43.116134Z",
     "iopub.status.busy": "2022-09-28T19:11:43.115135Z",
     "iopub.status.idle": "2022-09-28T19:11:43.127494Z",
     "shell.execute_reply": "2022-09-28T19:11:43.126225Z",
     "shell.execute_reply.started": "2022-09-28T19:11:43.116094Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_model(train_data, test_data):\n",
    "    \"\"\"Runs a long-short-term-memory neural net with 2 dense layers. \n",
    "    Generates predictions that are then unscaled. \n",
    "    Scores are printed and the results are plotted and saved.\"\"\"\n",
    "    # train_data: dataset used to train the model\n",
    "    # test_data: dataset used to test the model\n",
    "   \n",
    "    \n",
    "    # Split into X & y and scale data:\n",
    "    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "   \n",
    "    \n",
    "    # Build LSTM:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), \n",
    "                   stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, \n",
    "              shuffle=False)\n",
    "    predictions = model.predict(X_test,batch_size=1)\n",
    "    \n",
    "    # Undo scaling to compare predictions against original data:\n",
    "    origin_df = m_df\n",
    "    unscaled = re_scaling(predictions, X_test, scaler_object, lstm=True)\n",
    "    unscaled_df = prediction_df(unscaled, origin_df)\n",
    "    \n",
    "    get_scores(unscaled_df, origin_df, 'LSTM')\n",
    "    plot_results(unscaled_df, origin_df, 'LSTM')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:43.129098Z",
     "iopub.status.busy": "2022-09-28T19:11:43.128776Z",
     "iopub.status.idle": "2022-09-28T19:11:49.665515Z",
     "shell.execute_reply": "2022-09-28T19:11:49.664346Z",
     "shell.execute_reply.started": "2022-09-28T19:11:43.12907Z"
    }
   },
   "outputs": [],
   "source": [
    "lstm_model(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:49.667068Z",
     "iopub.status.busy": "2022-09-28T19:11:49.666757Z",
     "iopub.status.idle": "2022-09-28T19:11:49.673125Z",
     "shell.execute_reply": "2022-09-28T19:11:49.671939Z",
     "shell.execute_reply.started": "2022-09-28T19:11:49.667037Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_scores, open( \"model_scores.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling\n",
    "\n",
    "[ **SARIMAX Modeling**  ](https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/)\n",
    "\n",
    "> We use the statsmodels SARIMAX package to train the model and generate dynamic predictions. The SARIMA model breaks down into a few parts.\n",
    "\n",
    "     AR: represented as p, is the autoregressive model\n",
    "     I : represented as d, is the differencing term\n",
    "     MA: represented as q, is the moving average model\n",
    "     S: enables us to add a seasonal component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:49.675389Z",
     "iopub.status.busy": "2022-09-28T19:11:49.675043Z",
     "iopub.status.idle": "2022-09-28T19:11:49.685938Z",
     "shell.execute_reply": "2022-09-28T19:11:49.684531Z",
     "shell.execute_reply.started": "2022-09-28T19:11:49.675358Z"
    }
   },
   "outputs": [],
   "source": [
    "datatime_df.index = pd.to_datetime(datatime_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style =\"background-color : #e6ebef;\">\n",
    "    <p style=\"padding: 15px;\n",
    "              color:black;\">ðŸ“Œ In the code below, we define our model and then make dynamic predictions for the last 12 months of the data. For standard, non-dynamic predictions, the following monthâ€™s prediction is made using the actual sales from the prior months. In contrast, for dynamic predictions, the following monthâ€™s prediction is made using the predicted sales from the prior months.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:39:01.682986Z",
     "iopub.status.busy": "2022-09-28T19:39:01.681763Z",
     "iopub.status.idle": "2022-09-28T19:39:01.693573Z",
     "shell.execute_reply": "2022-09-28T19:39:01.691874Z",
     "shell.execute_reply.started": "2022-09-28T19:39:01.682941Z"
    }
   },
   "outputs": [],
   "source": [
    "def sarimax_model(data):\n",
    "    # Model:\n",
    "    sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(12, 0, 0),\n",
    "                                    seasonal_order=(0, 1, 0, 12),\n",
    "                                    trend='c').fit()\n",
    "    \n",
    "    # Generate predictions:\n",
    "    start, end, dynamic = 40, 100, 7\n",
    "    data['pred_value'] = sar.predict(start=start, end=end, dynamic=dynamic)\n",
    "    pred_df = data.pred_value[start+dynamic:end]\n",
    "    \n",
    "    data[[\"sales_diff\",\"pred_value\"]].plot(color=['blue', 'Red'])\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    model_score = {}\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(data.sales_diff[-12:], data.pred_value[-12:]))\n",
    "    mae = mean_absolute_error(data.sales_diff[-12:], data.pred_value[-12:])\n",
    "    r2 = r2_score(data.sales_diff[-12:], data.pred_value[-12:])\n",
    "    model_scores['ARIMA'] = [rmse, mae, r2]\n",
    "    \n",
    "    print(f\"RMSE: {rmse}\\nMAE: {mae}\\nR2 Score: {r2}\")\n",
    "    \n",
    "    return sar, data, pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:38:39.621394Z",
     "iopub.status.busy": "2022-09-28T19:38:39.62095Z",
     "iopub.status.idle": "2022-09-28T19:38:39.675106Z",
     "shell.execute_reply": "2022-09-28T19:38:39.67338Z",
     "shell.execute_reply.started": "2022-09-28T19:38:39.62136Z"
    }
   },
   "outputs": [],
   "source": [
    "sar, datatime_df, predictions = sarimax_model(datatime_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima.model.ARIMAResults.plot_diagnostics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:51.028531Z",
     "iopub.status.busy": "2022-09-28T19:11:51.028095Z",
     "iopub.status.idle": "2022-09-28T19:11:52.006526Z",
     "shell.execute_reply": "2022-09-28T19:11:52.005187Z",
     "shell.execute_reply.started": "2022-09-28T19:11:51.028496Z"
    }
   },
   "outputs": [],
   "source": [
    "sar.plot_diagnostics(figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.008493Z",
     "iopub.status.busy": "2022-09-28T19:11:52.008102Z",
     "iopub.status.idle": "2022-09-28T19:11:52.014205Z",
     "shell.execute_reply": "2022-09-28T19:11:52.012645Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.008452Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_scores, open( \"ARIMAmodel_scores.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparing Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.016452Z",
     "iopub.status.busy": "2022-09-28T19:11:52.016003Z",
     "iopub.status.idle": "2022-09-28T19:11:52.027589Z",
     "shell.execute_reply": "2022-09-28T19:11:52.026367Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.016392Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_results_df():\n",
    "    results_dict = pickle.load(open(\"model_scores.p\", \"rb\"))\n",
    "    \n",
    "    results_dict.update(pickle.load(open(\"ARIMAmodel_scores.p\", \"rb\")))\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient='index', \n",
    "                                        columns=['RMSE', 'MAE','R2'])\n",
    "    \n",
    "    results_df = results_df.sort_values(by='RMSE', ascending=False).reset_index()\n",
    "    \n",
    "    results_df.to_csv('./results.csv')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    sns.lineplot(np.arange(len(results_df)), 'RMSE', data=results_df, ax=ax, \n",
    "                 label='RMSE', color='darkblue')\n",
    "    sns.lineplot(np.arange(len(results_df)), 'MAE', data=results_df, ax=ax, \n",
    "                 label='MAE', color='Cyan')\n",
    "    \n",
    "    plt.xticks(np.arange(len(results_df)),rotation=45)\n",
    "    ax.set_xticklabels(results_df['index'])\n",
    "    ax.set(xlabel = \"Model\",\n",
    "           ylabel = \"Scores\",\n",
    "           title = \"Model Error Comparison\")\n",
    "    sns.despine()\n",
    "    \n",
    "    plt.savefig(f'./model_output/compare_models.png')\n",
    "    \n",
    "    return results_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.03017Z",
     "iopub.status.busy": "2022-09-28T19:11:52.029074Z",
     "iopub.status.idle": "2022-09-28T19:11:52.397584Z",
     "shell.execute_reply": "2022-09-28T19:11:52.396285Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.030124Z"
    }
   },
   "outputs": [],
   "source": [
    "results = create_results_df()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.399598Z",
     "iopub.status.busy": "2022-09-28T19:11:52.398881Z",
     "iopub.status.idle": "2022-09-28T19:11:52.405747Z",
     "shell.execute_reply": "2022-09-28T19:11:52.404747Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.399563Z"
    }
   },
   "outputs": [],
   "source": [
    "avarage_12months()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-28T19:11:52.407405Z",
     "iopub.status.busy": "2022-09-28T19:11:52.407057Z",
     "iopub.status.idle": "2022-09-28T19:11:52.416874Z",
     "shell.execute_reply": "2022-09-28T19:11:52.415835Z",
     "shell.execute_reply.started": "2022-09-28T19:11:52.40737Z"
    }
   },
   "outputs": [],
   "source": [
    "average = 894478.3333333334\n",
    "XGBoost = results.MAE.values[4]\n",
    "percentage_off = round(XGBoost/average*100,2)\n",
    "\n",
    "print(f\"With XGBoost, prediction is within {percentage_off}% of the actual.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868225,
     "sourceId": 9999,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30235,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
